{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcIz4Igd6Ae6"
   },
   "source": [
    "# Model Distillation for Pretrained LLMs\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Introduction to Model Distillation**\n",
    "\n",
    "**Model distillation** is a model compression technique in which a smaller, simpler model (the *student*) is trained to replicate the behavior of a larger, often more accurate model (the *teacher*). \n",
    "\n",
    "- Introduced in the paper \"Distilling the Knowledge in a Neural Network\" by Geoffrey Hinton, Oriol Vinyals, Jeff Deanm (2015) https://arxiv.org/abs/1503.02531\n",
    "\n",
    "- Distillation is a cornerstone in the development of resource-efficient machine learning systems.\n",
    "\n",
    "In traditional supervised learning, models learn from labeled data. In contrast, model distillation uses *soft targets*, the probabilistic outputs (logits) of a teacher model to transfer knowledge. This helps the student model learn not just the correct answer but the *relative confidence* in each possible output, capturing rich information about the decision boundary.\n",
    "\n",
    "### Benefits of Model Distillation\n",
    "\n",
    "* Reduces **inference latency** and **memory footprint**\n",
    "* Makes deployment to **edge devices** feasible\n",
    "* Allows **faster training and inference** for downstream tasks\n",
    "* Improves **generalization** by capturing teacher’s inductive biases\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Model Distillation for Pretrained Large Language Models (LLMs)**\n",
    "\n",
    "- LLMs such as GPT, BERT, LLaMA, and Falcon have billions of parameters and require immense resources for inference and fine-tuning. \n",
    "- Distillation makes it possible to produce smaller, faster models while retaining much of the performance, enabling:\n",
    "\n",
    "* Real-time inference in production\n",
    "* Use in resource-constrained environments\n",
    "* Democratization of powerful LLMs to broader audiences\n",
    "\n",
    "### Types of Distillation in LLMs\n",
    "\n",
    "| Type                  | Description                                                     |\n",
    "| --------------------- | --------------------------------------------------------------- |\n",
    "| **Logit-based**       | Student mimics teacher’s output probabilities (most common)     |\n",
    "| **Feature-based**     | Student mimics hidden representations from intermediate layers  |\n",
    "| **Task-specific**     | Student learns from teacher on a specific downstream task       |\n",
    "| **Self-distillation** | Teacher and student are the same model at different checkpoints |\n",
    "| **Multi-teacher**     | Combines knowledge from multiple teacher models                 |\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Best Practices in LLM Distillation**\n",
    "\n",
    "### a. **Pretraining vs. Finetuning Distillation**\n",
    "\n",
    "* **Pretraining Distillation**: Distilling a model over the same corpus as the teacher's original pretraining (rare due to cost).\n",
    "* **Post-hoc or Finetuning Distillation**: Much more common; distills a finetuned teacher (e.g., on QA or summarization) to a smaller student.\n",
    "\n",
    "### b. **Distillation Objective Functions**\n",
    "\n",
    "* **Kullback–Leibler Divergence (KL-div)**: Most common loss function for matching teacher logits.\n",
    "* **Cosine similarity / MSE**: For feature-based distillation.\n",
    "* **Combined Loss**: Mix of supervised loss (e.g., cross-entropy with ground truth) and distillation loss with teacher outputs.\n",
    "\n",
    "### c. **Temperature Scaling**\n",
    "\n",
    "* Use of a *temperature parameter* > 1 softens teacher logits, exposing more useful information for student learning.\n",
    "\n",
    "### d. **Curriculum Learning**\n",
    "\n",
    "* Begin training on easy examples, then progressively introduce harder ones to stabilize training.\n",
    "\n",
    "### e. **Layer Mapping**\n",
    "\n",
    "* Choose a strategy to align teacher and student layers. Common ones:\n",
    "\n",
    "  * Match final outputs only\n",
    "  * Match every Nth layer\n",
    "  * Match all corresponding layers (if architectures align)\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Evaluating Distilled Models**\n",
    "\n",
    "### a. **Performance Metrics**\n",
    "\n",
    "* Compare student vs. teacher on:\n",
    "\n",
    "  * **Accuracy / F1 / BLEU / ROUGE** (task-specific)\n",
    "  * **Log-likelihood / perplexity** (language modeling)\n",
    "  * **Win rate in pairwise evaluations** (for generation tasks)\n",
    "\n",
    "### b. **Behavioral Parity Tests**\n",
    "\n",
    "* Check if the student preserves qualitative behavior like:\n",
    "\n",
    "  * Instruction-following\n",
    "  * Ethical constraints\n",
    "  * Biases (may be desirable to remove)\n",
    "\n",
    "### c. **Distillation Quality Indicators**\n",
    "\n",
    "* High KL divergence = poor distillation\n",
    "* Low task accuracy despite similar logits = underfitting\n",
    "* Similar output distribution and behavior = good distillation\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Dataset Considerations**\n",
    "\n",
    "### a. **Do You Need a Dataset?**\n",
    "\n",
    "* **Yes**, especially for task-specific or fine-tuning distillation.\n",
    "* **Unlabeled data** is often sufficient when using teacher outputs (i.e., pseudo-labels).\n",
    "* Some self-distillation strategies or synthetic datasets can reduce need for large-scale corpora.\n",
    "\n",
    "### b. **Choosing the Right Dataset**\n",
    "\n",
    "* **For general-purpose LLMs**:\n",
    "\n",
    "  * Use a corpus with wide domain coverage (e.g., C4, OpenWebText)\n",
    "* **For downstream tasks**:\n",
    "\n",
    "  * Use the same dataset teacher was fine-tuned on (e.g., SQuAD for QA)\n",
    "* **For instruction tuning**:\n",
    "\n",
    "  * Use datasets like Alpaca, Dolly, or FLAN-style prompts\n",
    "\n",
    "### c. **Data Augmentation Techniques**\n",
    "\n",
    "* Paraphrasing\n",
    "* Prompt variation\n",
    "* Synthetic data generation using teacher\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Resource Requirements**\n",
    "\n",
    "| Component         | Requirement                                                         |\n",
    "| ----------------- | ------------------------------------------------------------------- |\n",
    "| **Teacher model** | GPU/TPU for inference, ideally batched                              |\n",
    "| **Student model** | Moderate-sized GPU/TPU (can be trained with less memory)            |\n",
    "| **Storage**       | For intermediate logits if storing teacher outputs                  |\n",
    "| **Compute**       | Depends on student size and dataset; less than full LLM pretraining |\n",
    "\n",
    "* Example: Distilling GPT-3 (175B) into a 6B model may require:\n",
    "\n",
    "  * 2–4 A100s (teacher inference)\n",
    "  * 1–2 A100s (student training)\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Success Stories in LLM Distillation**\n",
    "\n",
    "* **DistilBERT** (Sanh et al., 2019): 40% smaller, 60% faster, 97% performance retention from BERT\n",
    "* **TinyBERT**: Layer-to-layer distillation with task-specific finetuning\n",
    "* **MiniLM**: Distilled with deep self-attention distillation, performs close to BERT with fewer parameters\n",
    "* **DistilGPT2**: Smaller GPT2 variant used in real-time applications\n",
    "* **Alpaca & Vicuna**: Instruction-tuned smaller models distilled from LLaMA using curated prompt datasets\n",
    "* **Mistral 7B Instruct**: Partly distilled, instruction-tuned model close to GPT-3.5\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Common Pitfalls**\n",
    "\n",
    "### a. **Overfitting to Teacher**\n",
    "\n",
    "* Student may memorize outputs instead of generalizing. Remedy: include ground truth loss or use diverse data.\n",
    "\n",
    "### b. **Poor Dataset Coverage**\n",
    "\n",
    "* If student is trained on narrow or biased data, it may fail to generalize or hallucinate.\n",
    "\n",
    "### c. **Misaligned Architectures**\n",
    "\n",
    "* Difficult to align hidden states if student and teacher use different architectures.\n",
    "\n",
    "### d. **Loss of Calibration**\n",
    "\n",
    "* Distillation can worsen uncertainty estimation (e.g., confidence scores), affecting downstream trust.\n",
    "\n",
    "### e. **Evaluation Gaps**\n",
    "\n",
    "* Student may perform well on benchmarks but poorly in open-ended or out-of-distribution scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Tools and Libraries**\n",
    "\n",
    "* **Hugging Face Transformers + Datasets**\n",
    "* **PyTorch + PyTorch Lightning**\n",
    "* **OpenDelta / LoRA**: Efficient fine-tuning strategies usable with distillation\n",
    "* **DistillToolkit**: Community projects for distillation (e.g., FastDistill)\n",
    "* **LMFlow**: Open-source framework for LLM distillation\n",
    "* **DeepSpeed / Megatron-LM**: Efficient parallelism for large-scale distillation\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Distilling a Pre-trained LLM using TensorFlow\n",
    "- ### SQuAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import TFAutoModelForQuestionAnswering, AutoTokenizer, DefaultDataCollator\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit memory usage\n",
    "- Not required while using a CPU only small compute instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "tf.config.threading.set_intra_op_parallelism_threads(2)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the SQuAD dataset\n",
    "\n",
    "* **SQuAD (Stanford Question Answering Dataset)** is a benchmark dataset for machine reading comprehension and question answering.\n",
    "* Contains **Wikipedia paragraphs** with accompanying **questions and answers**, where the answer is a span from the passage.\n",
    "* **SQuAD v1.1** has \\~100K questions with answers guaranteed to be in the text; **v2.0** adds \\~50K unanswerable questions.\n",
    "* Widely used to **train and evaluate QA models**, especially transformer-based architectures like BERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = load_dataset(\"squad\")\n",
    "\n",
    "# Use a small subset for resource constraints\n",
    "# You can limit the training set further to 500\n",
    "# Keep in mind that training a student on a small dataset is for demo purposes only\n",
    "train_data = squad['train'].select(range(1000))\n",
    "val_data = squad['validation'].select(range(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the teacher model and tokenizer\n",
    "\n",
    "**BERT base uncased** is a popular pretrained transformer model:\n",
    "* **12 layers**, **768 hidden units**, and **12 attention heads**, totaling **110 million parameters**.\n",
    "* **\"Uncased\"** means all input text is lowercased and the model doesn't distinguish between uppercase and lowercase letters.\n",
    "* Pretrained on **BooksCorpus and English Wikipedia** using masked language modeling (MLM) and next sentence prediction (NSP).\n",
    "* Suitable for many downstream NLP tasks via fine-tuning, such as classification, QA, and NER.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_name = \"bert-base-uncased\"\n",
    "teacher_model = TFAutoModelForQuestionAnswering.from_pretrained(teacher_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data\n",
    "\n",
    "* **Tokenize** each `(question, context)` pair using a sliding window to handle long contexts (`stride=128`, `max_length=384`). If the context is longer than max_length, it must be split into overlapping chunks so that all parts of the long context are covered. `stride` indicates overlapping tokens.  \n",
    "* **Track answer positions**: Computes the token start and end positions of the correct answer span in the tokenized context using character-to-token mapping.\n",
    "* **Handle edge cases**: If no answer is present, the model is trained to predict the `[CLS]` token position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
    "        input_ids = inputs[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        answer = answers[sample_index]\n",
    "        if len(answer[\"answer_start\"]) == 0:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answer[\"text\"][0])\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "            if not (offset[token_start_index][0] <= start_char and offset[token_end_index][1] >= end_char):\n",
    "                start_positions.append(cls_index)\n",
    "                end_positions.append(cls_index)\n",
    "            else:\n",
    "                while token_start_index < len(offset) and offset[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                start_positions.append(token_start_index - 1)\n",
    "                while offset[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                end_positions.append(token_end_index + 1)\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "encoded_train = train_data.map(preprocess_function, batched=True, remove_columns=train_data.column_names)\n",
    "encoded_val = val_data.map(preprocess_function, batched=True, remove_columns=val_data.column_names)\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert datasets to tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_dataset = encoded_train.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"start_positions\", \"end_positions\"],\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "val_tf_dataset = encoded_val.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"start_positions\", \"end_positions\"],\n",
    "    shuffle=False,\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the student model (smaller architecture)\n",
    "\n",
    "\n",
    "* Create a **smaller student BERT model** that mimics the architecture of a pretrained teacher, but with fewer layers. Ideal for **distillation**.\n",
    "* `BertConfig`: Manages model architecture settings.\n",
    "* `TFBertForQuestionAnswering`: A TensorFlow model class pre-built for QA tasks.\n",
    "* For the Student Model, we will manually set the number of transformer encoder layers to 4 (original BERT has 12).\n",
    "   * This makes the model smaller and faster: ideal for **knowledge distillation**.\n",
    "\n",
    "* **Create the student model:**\n",
    "\n",
    "  `student_model = TFBertForQuestionAnswering(config=student_config)`\n",
    "\n",
    "   * Build a **new BERT QA model** with the modified (shallow) architecture.\n",
    "   * Weights are randomly initialized—this student will typically be trained (or distilled) from the teacher.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, TFBertForQuestionAnswering\n",
    "\n",
    "student_config = BertConfig.from_pretrained(teacher_model_name)\n",
    "student_config.num_hidden_layers = 4  # Reduce depth\n",
    "student_model = TFBertForQuestionAnswering(config=student_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distillation loss (teacher-student)\n",
    "\n",
    "- Let's build a **custom TensorFlow Keras model class for knowledge distillation** in a **question answering (QA)** task. \n",
    "\n",
    "- The **student model** needs to learn both:\n",
    "\n",
    "    - **from ground-truth labels (hard targets)**\n",
    "    - **from teacher model outputs (soft targets)**\n",
    "\n",
    "* **`loss_fn_hard`**:\n",
    "  Computes loss using true labels\n",
    "  → `SparseCategoricalCrossentropy` (for start/end position labels)\n",
    "\n",
    "* **`loss_fn_soft`**:\n",
    "  Computes KL divergence between teacher and student predictions\n",
    "  → Used for soft-target distillation\n",
    "\n",
    "* `alpha`: weight factor between hard and soft losses\n",
    "\n",
    "##### `train_step(self, data)`\n",
    "\n",
    "* Custom training logic:\n",
    "\n",
    "  * Gets outputs from both teacher and student\n",
    "  * Computes:\n",
    "\n",
    "    * **Hard loss**: student vs ground truth\n",
    "    * **Soft loss**: student vs teacher\n",
    "  * Combines them using `alpha`\n",
    "  * Backpropagates only on student\n",
    "\n",
    "##### `test_step(self, data)`\n",
    "\n",
    "* Evaluation step (uses only hard loss with ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification loss function so that the model can learn from the labels\n",
    "loss_fn_hard = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# additionally, we need KLD loss because the student also needs to learn from logits (probabilities)\n",
    "loss_fn_soft = tf.keras.losses.KLDivergence()\n",
    "\n",
    "\n",
    "\n",
    "class DistillationModel(tf.keras.Model):\n",
    "    def __init__(self, student, teacher, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.student = student\n",
    "        self.teacher = teacher\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def compile(self, optimizer):\n",
    "        super().compile()\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            student_outputs = self.student(x, training=True)\n",
    "            teacher_outputs = self.teacher(x, training=False)\n",
    "\n",
    "            s_start = student_outputs.start_logits\n",
    "            s_end = student_outputs.end_logits\n",
    "            t_start = teacher_outputs.start_logits\n",
    "            t_end = teacher_outputs.end_logits\n",
    "\n",
    "            y_start = tf.reshape(y[\"start_positions\"], [-1])\n",
    "            y_end = tf.reshape(y[\"end_positions\"], [-1])\n",
    "\n",
    "            loss_start_hard = loss_fn_hard(y_start, s_start)\n",
    "            loss_end_hard = loss_fn_hard(y_end, s_end)\n",
    "\n",
    "            loss_start_soft = loss_fn_soft(\n",
    "                tf.nn.softmax(t_start, axis=-1),\n",
    "                tf.nn.log_softmax(s_start, axis=-1)\n",
    "            )\n",
    "            loss_end_soft = loss_fn_soft(\n",
    "                tf.nn.softmax(t_end, axis=-1),\n",
    "                tf.nn.log_softmax(s_end, axis=-1)\n",
    "            )\n",
    "\n",
    "            loss_start = self.alpha * loss_start_hard + (1 - self.alpha) * loss_start_soft\n",
    "            loss_end = self.alpha * loss_end_hard + (1 - self.alpha) * loss_end_soft\n",
    "\n",
    "            loss = (loss_start + loss_end) / 2\n",
    "\n",
    "        grads = tape.gradient(loss, self.student.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.student.trainable_variables))\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        student_outputs = self.student(x, training=False)\n",
    "        s_start = student_outputs.start_logits\n",
    "        s_end = student_outputs.end_logits\n",
    "        y_start = tf.reshape(y[\"start_positions\"], [-1])\n",
    "        y_end = tf.reshape(y[\"end_positions\"], [-1])\n",
    "        loss_start = loss_fn_hard(y_start, s_start)\n",
    "        loss_end = loss_fn_hard(y_end, s_end)\n",
    "        loss = (loss_start + loss_end) / 2\n",
    "        return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the distilled model\n",
    "- Note that beyond the demo lab, you will need to budget research time to optimize the hyperparameters such as:\n",
    "    - alpha\n",
    "    - learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distilled_model = DistillationModel(student_model, teacher_model, alpha=0.7)\n",
    "distilled_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    ")\n",
    "distilled_model.fit(train_tf_dataset, epochs=2, validation_data=val_tf_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate and compare size and speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYBGOavgPGGw"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tempfile\n",
    "\n",
    "# Measure inference time\n",
    "sample = next(iter(val_tf_dataset))\n",
    "start = time.time()\n",
    "_ = student_model(sample[0])\n",
    "end = time.time()\n",
    "print(\"Student inference time (batch):\", end - start)\n",
    "\n",
    "start = time.time()\n",
    "_ = teacher_model(sample[0])\n",
    "end = time.time()\n",
    "print(\"Teacher inference time (batch):\", end - start)\n",
    "\n",
    "# Model size comparison\n",
    "student_model.save_pretrained(tempfile.mkdtemp())\n",
    "teacher_model.save_pretrained(tempfile.mkdtemp())\n",
    "\n",
    "print(\"Student model size (MB):\", sum(os.path.getsize(os.path.join(root, f)) for root, _, files in os.walk(student_model.name_or_path) for f in files) / 1e6)\n",
    "print(\"Teacher model size (MB):\", sum(os.path.getsize(os.path.join(root, f)) for root, _, files in os.walk(teacher_model.name_or_path) for f in files) / 1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYxidPju550S"
   },
   "outputs": [],
   "source": [
    "student_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.save(\"student_model\")\n",
    "teacher_model.save(\"teacher_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
