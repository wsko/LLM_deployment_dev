{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcIz4Igd6Ae6"
   },
   "source": [
    "# Model Distillation for Pretrained LLMs\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Introduction to Model Distillation**\n",
    "\n",
    "**Model distillation** is a model compression technique in which a smaller, simpler model (the *student*) is trained to replicate the behavior of a larger, often more accurate model (the *teacher*). Introduced by Hinton et al. (2015), distillation has become a cornerstone in the development of resource-efficient machine learning systems.\n",
    "\n",
    "In traditional supervised learning, models learn from labeled data. In contrast, model distillation uses *soft targets*—the probabilistic outputs (logits) of a teacher model—to transfer knowledge. This helps the student learn not just the correct answer but the *relative confidence* in each possible output, capturing rich information about the decision boundary.\n",
    "\n",
    "### Benefits of Model Distillation\n",
    "\n",
    "* Reduces **inference latency** and **memory footprint**\n",
    "* Makes deployment to **edge devices** feasible\n",
    "* Allows **faster training and inference** for downstream tasks\n",
    "* Improves **generalization** by capturing teacher’s inductive biases\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Model Distillation for Pretrained Large Language Models (LLMs)**\n",
    "\n",
    "LLMs such as GPT, BERT, LLaMA, and Falcon have billions of parameters and require immense resources for inference and fine-tuning. Distillation makes it possible to produce smaller, faster models while retaining much of the performance, enabling:\n",
    "\n",
    "* Real-time inference in production\n",
    "* Use in resource-constrained environments\n",
    "* Democratization of powerful LLMs to broader audiences\n",
    "\n",
    "### Types of Distillation in LLMs\n",
    "\n",
    "| Type                  | Description                                                     |\n",
    "| --------------------- | --------------------------------------------------------------- |\n",
    "| **Logit-based**       | Student mimics teacher’s output probabilities (most common)     |\n",
    "| **Feature-based**     | Student mimics hidden representations from intermediate layers  |\n",
    "| **Task-specific**     | Student learns from teacher on a specific downstream task       |\n",
    "| **Self-distillation** | Teacher and student are the same model at different checkpoints |\n",
    "| **Multi-teacher**     | Combines knowledge from multiple teacher models                 |\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Best Practices in LLM Distillation**\n",
    "\n",
    "### a. **Pretraining vs. Finetuning Distillation**\n",
    "\n",
    "* **Pretraining Distillation**: Distilling a model over the same corpus as the teacher's original pretraining (rare due to cost).\n",
    "* **Post-hoc or Finetuning Distillation**: Much more common; distills a finetuned teacher (e.g., on QA or summarization) to a smaller student.\n",
    "\n",
    "### b. **Distillation Objective Functions**\n",
    "\n",
    "* **Kullback–Leibler Divergence (KL-div)**: Most common loss function for matching teacher logits.\n",
    "* **Cosine similarity / MSE**: For feature-based distillation.\n",
    "* **Combined Loss**: Mix of supervised loss (e.g., cross-entropy with ground truth) and distillation loss with teacher outputs.\n",
    "\n",
    "### c. **Temperature Scaling**\n",
    "\n",
    "* Use of a *temperature parameter* > 1 softens teacher logits, exposing more useful information for student learning.\n",
    "\n",
    "### d. **Curriculum Learning**\n",
    "\n",
    "* Begin training on easy examples, then progressively introduce harder ones to stabilize training.\n",
    "\n",
    "### e. **Layer Mapping**\n",
    "\n",
    "* Choose a strategy to align teacher and student layers. Common ones:\n",
    "\n",
    "  * Match final outputs only\n",
    "  * Match every Nth layer\n",
    "  * Match all corresponding layers (if architectures align)\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Evaluating Distilled Models**\n",
    "\n",
    "### a. **Performance Metrics**\n",
    "\n",
    "* Compare student vs. teacher on:\n",
    "\n",
    "  * **Accuracy / F1 / BLEU / ROUGE** (task-specific)\n",
    "  * **Log-likelihood / perplexity** (language modeling)\n",
    "  * **Win rate in pairwise evaluations** (for generation tasks)\n",
    "\n",
    "### b. **Behavioral Parity Tests**\n",
    "\n",
    "* Check if the student preserves qualitative behavior like:\n",
    "\n",
    "  * Instruction-following\n",
    "  * Ethical constraints\n",
    "  * Biases (may be desirable to remove)\n",
    "\n",
    "### c. **Distillation Quality Indicators**\n",
    "\n",
    "* High KL divergence = poor distillation\n",
    "* Low task accuracy despite similar logits = underfitting\n",
    "* Similar output distribution and behavior = good distillation\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Dataset Considerations**\n",
    "\n",
    "### a. **Do You Need a Dataset?**\n",
    "\n",
    "* **Yes**, especially for task-specific or fine-tuning distillation.\n",
    "* **Unlabeled data** is often sufficient when using teacher outputs (i.e., pseudo-labels).\n",
    "* Some self-distillation strategies or synthetic datasets can reduce need for large-scale corpora.\n",
    "\n",
    "### b. **Choosing the Right Dataset**\n",
    "\n",
    "* **For general-purpose LLMs**:\n",
    "\n",
    "  * Use a corpus with wide domain coverage (e.g., C4, OpenWebText)\n",
    "* **For downstream tasks**:\n",
    "\n",
    "  * Use the same dataset teacher was fine-tuned on (e.g., SQuAD for QA)\n",
    "* **For instruction tuning**:\n",
    "\n",
    "  * Use datasets like Alpaca, Dolly, or FLAN-style prompts\n",
    "\n",
    "### c. **Data Augmentation Techniques**\n",
    "\n",
    "* Paraphrasing\n",
    "* Prompt variation\n",
    "* Synthetic data generation using teacher\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Resource Requirements**\n",
    "\n",
    "| Component         | Requirement                                                         |\n",
    "| ----------------- | ------------------------------------------------------------------- |\n",
    "| **Teacher model** | GPU/TPU for inference, ideally batched                              |\n",
    "| **Student model** | Moderate-sized GPU/TPU (can be trained with less memory)            |\n",
    "| **Storage**       | For intermediate logits if storing teacher outputs                  |\n",
    "| **Compute**       | Depends on student size and dataset; less than full LLM pretraining |\n",
    "\n",
    "* Example: Distilling GPT-3 (175B) into a 6B model may require:\n",
    "\n",
    "  * 2–4 A100s (teacher inference)\n",
    "  * 1–2 A100s (student training)\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Success Stories in LLM Distillation**\n",
    "\n",
    "* **DistilBERT** (Sanh et al., 2019): 40% smaller, 60% faster, 97% performance retention from BERT\n",
    "* **TinyBERT**: Layer-to-layer distillation with task-specific finetuning\n",
    "* **MiniLM**: Distilled with deep self-attention distillation, performs close to BERT with fewer parameters\n",
    "* **DistilGPT2**: Smaller GPT2 variant used in real-time applications\n",
    "* **Alpaca & Vicuna**: Instruction-tuned smaller models distilled from LLaMA using curated prompt datasets\n",
    "* **Mistral 7B Instruct**: Partly distilled, instruction-tuned model close to GPT-3.5\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Common Pitfalls**\n",
    "\n",
    "### a. **Overfitting to Teacher**\n",
    "\n",
    "* Student may memorize outputs instead of generalizing. Remedy: include ground truth loss or use diverse data.\n",
    "\n",
    "### b. **Poor Dataset Coverage**\n",
    "\n",
    "* If student is trained on narrow or biased data, it may fail to generalize or hallucinate.\n",
    "\n",
    "### c. **Misaligned Architectures**\n",
    "\n",
    "* Difficult to align hidden states if student and teacher use different architectures.\n",
    "\n",
    "### d. **Loss of Calibration**\n",
    "\n",
    "* Distillation can worsen uncertainty estimation (e.g., confidence scores), affecting downstream trust.\n",
    "\n",
    "### e. **Evaluation Gaps**\n",
    "\n",
    "* Student may perform well on benchmarks but poorly in open-ended or out-of-distribution scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Tools and Libraries**\n",
    "\n",
    "* **Hugging Face Transformers + Datasets**\n",
    "* **PyTorch + PyTorch Lightning**\n",
    "* **OpenDelta / LoRA**: Efficient fine-tuning strategies usable with distillation\n",
    "* **DistillToolkit**: Community projects for distillation (e.g., FastDistill)\n",
    "* **LMFlow**: Open-source framework for LLM distillation\n",
    "* **DeepSpeed / Megatron-LM**: Efficient parallelism for large-scale distillation\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Key Takeaways**\n",
    "\n",
    "Model distillation is an essential tool for making LLMs practical and scalable. It enables smaller models to inherit the capabilities of their larger counterparts with significant savings in computational cost and latency. By carefully selecting the dataset, aligning training objectives, and monitoring the student’s performance, it’s possible to build powerful, distilled models that retain the essence of foundational LLMs.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Distilling a Pre-trained LLM using TensorFlow\n",
    "- ### SQuAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import TFAutoModelForQuestionAnswering, AutoTokenizer, DefaultDataCollator\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "tf.config.threading.set_intra_op_parallelism_threads(2)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the SQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = load_dataset(\"squad\")\n",
    "\n",
    "# Use a small subset for resource constraints\n",
    "train_data = squad['train'].select(range(1000))\n",
    "val_data = squad['validation'].select(range(200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the teacher model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_name = \"bert-base-uncased\"\n",
    "teacher_model = TFAutoModelForQuestionAnswering.from_pretrained(teacher_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
    "        input_ids = inputs[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        answer = answers[sample_index]\n",
    "        if len(answer[\"answer_start\"]) == 0:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answer[\"text\"][0])\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "            if not (offset[token_start_index][0] <= start_char and offset[token_end_index][1] >= end_char):\n",
    "                start_positions.append(cls_index)\n",
    "                end_positions.append(cls_index)\n",
    "            else:\n",
    "                while token_start_index < len(offset) and offset[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                start_positions.append(token_start_index - 1)\n",
    "                while offset[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                end_positions.append(token_end_index + 1)\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "encoded_train = train_data.map(preprocess_function, batched=True, remove_columns=train_data.column_names)\n",
    "encoded_val = val_data.map(preprocess_function, batched=True, remove_columns=val_data.column_names)\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert datasets to tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_dataset = encoded_train.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"start_positions\", \"end_positions\"],\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "val_tf_dataset = encoded_val.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"start_positions\", \"end_positions\"],\n",
    "    shuffle=False,\n",
    "    batch_size=8,\n",
    "    collate_fn=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the student model (smaller architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, TFBertForQuestionAnswering\n",
    "\n",
    "student_config = BertConfig.from_pretrained(teacher_model_name)\n",
    "student_config.num_hidden_layers = 4  # Reduce depth\n",
    "student_model = TFBertForQuestionAnswering(config=student_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distillation loss (teacher-student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_hard = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_fn_soft = tf.keras.losses.KLDivergence()\n",
    "\n",
    "class DistillationModel(tf.keras.Model):\n",
    "    def __init__(self, student, teacher, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.student = student\n",
    "        self.teacher = teacher\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def compile(self, optimizer):\n",
    "        super().compile()\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            student_outputs = self.student(x, training=True)\n",
    "            teacher_outputs = self.teacher(x, training=False)\n",
    "\n",
    "            s_start = student_outputs.start_logits\n",
    "            s_end = student_outputs.end_logits\n",
    "            t_start = teacher_outputs.start_logits\n",
    "            t_end = teacher_outputs.end_logits\n",
    "\n",
    "            y_start = tf.reshape(y[\"start_positions\"], [-1])\n",
    "            y_end = tf.reshape(y[\"end_positions\"], [-1])\n",
    "\n",
    "            loss_start_hard = loss_fn_hard(y_start, s_start)\n",
    "            loss_end_hard = loss_fn_hard(y_end, s_end)\n",
    "\n",
    "            loss_start_soft = loss_fn_soft(\n",
    "                tf.nn.softmax(t_start, axis=-1),\n",
    "                tf.nn.log_softmax(s_start, axis=-1)\n",
    "            )\n",
    "            loss_end_soft = loss_fn_soft(\n",
    "                tf.nn.softmax(t_end, axis=-1),\n",
    "                tf.nn.log_softmax(s_end, axis=-1)\n",
    "            )\n",
    "\n",
    "            loss_start = self.alpha * loss_start_hard + (1 - self.alpha) * loss_start_soft\n",
    "            loss_end = self.alpha * loss_end_hard + (1 - self.alpha) * loss_end_soft\n",
    "\n",
    "            loss = (loss_start + loss_end) / 2\n",
    "\n",
    "        grads = tape.gradient(loss, self.student.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.student.trainable_variables))\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        student_outputs = self.student(x, training=False)\n",
    "        s_start = student_outputs.start_logits\n",
    "        s_end = student_outputs.end_logits\n",
    "        y_start = tf.reshape(y[\"start_positions\"], [-1])\n",
    "        y_end = tf.reshape(y[\"end_positions\"], [-1])\n",
    "        loss_start = loss_fn_hard(y_start, s_start)\n",
    "        loss_end = loss_fn_hard(y_end, s_end)\n",
    "        loss = (loss_start + loss_end) / 2\n",
    "        return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the distilled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distilled_model = DistillationModel(student_model, teacher_model, alpha=0.7)\n",
    "distilled_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    ")\n",
    "distilled_model.fit(train_tf_dataset, epochs=2, validation_data=val_tf_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate and compare size and speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYBGOavgPGGw"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tempfile\n",
    "\n",
    "# Measure inference time\n",
    "sample = next(iter(val_tf_dataset))\n",
    "start = time.time()\n",
    "_ = student_model(sample[0])\n",
    "end = time.time()\n",
    "print(\"Student inference time (batch):\", end - start)\n",
    "\n",
    "start = time.time()\n",
    "_ = teacher_model(sample[0])\n",
    "end = time.time()\n",
    "print(\"Teacher inference time (batch):\", end - start)\n",
    "\n",
    "# Model size comparison\n",
    "student_model.save_pretrained(tempfile.mkdtemp())\n",
    "teacher_model.save_pretrained(tempfile.mkdtemp())\n",
    "\n",
    "print(\"Student model size (MB):\", sum(os.path.getsize(os.path.join(root, f)) for root, _, files in os.walk(student_model.name_or_path) for f in files) / 1e6)\n",
    "print(\"Teacher model size (MB):\", sum(os.path.getsize(os.path.join(root, f)) for root, _, files in os.walk(teacher_model.name_or_path) for f in files) / 1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYxidPju550S"
   },
   "outputs": [],
   "source": [
    "student_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.save(\"student_model\")\n",
    "teacher_model.save(\"teacher_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
