{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bba7bbd-1031-4333-ad97-0c729a861385",
   "metadata": {},
   "source": [
    "# Hackathon: Optimize and Deploy Text Summarization Model\n",
    "\n",
    "---\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "- Optimize a transformer-based text summarization model for efficient deployment in a resource-constrained environment to simulate a production scenario.\n",
    "- Use **knowledge distillation** and **manual pruning** techniques to reduce model size and inference latency without significantly compromising summarization quality.\n",
    "- Use a subset of the CNN/DailyMail dataset and fine-tune a student model (`t5-small`).\n",
    "- Apply structured weight pruning to further compress the model.\n",
    "- The final model is saved in TensorFlow format, suitable for containerized inference deployment.\n",
    "\n",
    "---\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. **Subsampling** you need to subsample the original CNN/DailyMail dataset to adjust it to the small compute resources\n",
    "2. **Distillation**: Train a compact student model to mimic the output behavior of a larger teacher model using both hard labels and soft logits.\n",
    "3. **Pruning**: Apply magnitude-based weight pruning manually to remove unimportant weights from the trained student model.\n",
    "4. **TensorFlow Only**: All components—models, training, pruning, and saving—must use TensorFlow (no PyTorch).\n",
    "5. **No Quantization**: Skip any quantization techniques in this notebook.\n",
    "6. **Deployment Ready**: Save the optimized model using TensorFlow’s `SavedModel` format for later use in Docker and ECS environments.\n",
    "7. **Bonus**: Experiment with subsample size, batch size and number of epochs.\n",
    "\n",
    "---\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "* The dataset is a pre-subsampled version of CNN/DailyMail (\\~200 training and \\~50 test examples or less) and is available locally in JSON format.\n",
    "* The selected models (`t5-base` and `t5-small`) are fully supported by Hugging Face's TensorFlow APIs.\n",
    "* Training is conducted on a CPU-only machine (e.g., AWS `t2.large`), requiring careful management of memory and runtime.\n",
    "* Only basic training (2 epochs) is required to demonstrate optimization strategies, not to reach state-of-the-art performance.\n",
    "* Final output will be integrated into a containerized microservice, so model size and inference efficiency are critical.\n",
    "\n",
    "---\n",
    "\n",
    "Here's a concise yet informative **description of the CNN/DailyMail dataset** that you can include in your notebook or project documentation:\n",
    "\n",
    "---\n",
    "\n",
    "## CNN/DailyMail Dataset Description\n",
    "\n",
    "The **CNN/DailyMail** dataset is a large-scale benchmark for **abstractive text summarization**, widely used in natural language processing research. It consists of news articles from CNN and the Daily Mail, paired with human-written summaries, often referred to as *highlights*.\n",
    "\n",
    "Each example in the dataset includes:\n",
    "\n",
    "* **`article`**: The full body of a news story (approx. 300–800 words).\n",
    "* **`highlights`**: A concise, human-curated summary capturing the main points (1–3 sentences).\n",
    "\n",
    "### Dataset Versions\n",
    "\n",
    "* The most commonly used version is **\"3.0.0\"**, which filters out anonymized entity tags and provides clean text.\n",
    "* The dataset is split into:\n",
    "\n",
    "  * `train`: \\~287,000 examples\n",
    "  * `validation`: \\~13,000 examples\n",
    "  * `test`: \\~11,000 examples\n",
    "\n",
    "### Use Case in This Project\n",
    "\n",
    "In this project, we use a **subsampled version** of the dataset (e.g., 200 train, 50 test examples) to simulate summarization model training in a resource-constrained environment. The summaries serve as target outputs for fine-tuning a student model using knowledge distillation techniques.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aed08fe-a249-45af-9a41-14f0c048b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Imports ---\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# --- Load and prepare data ---\n",
    "with open(\"subsampled_cnn_dailymail/train_sample.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"subsampled_cnn_dailymail/test_sample.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35eeb6c3-8ce5-4f2d-b6dc-791d6dd3b60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Parameters ---\n",
    "MODEL_NAME_TEACHER = \"t5-base\"\n",
    "MODEL_NAME_STUDENT = \"t5-small\"\n",
    "MAX_INPUT_LEN = 512\n",
    "MAX_TARGET_LEN = 64\n",
    "BATCH_SIZE = 16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ce4bb59-ff7c-4865-8d34-8d5165155fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_STUDENT)\n",
    "\n",
    "def preprocess(example):\n",
    "    inputs = tokenizer(\n",
    "        example[\"article\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_LEN,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        example[\"summary\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_TARGET_LEN,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"][0],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"][0],\n",
    "        \"labels\": targets[\"input_ids\"][0],\n",
    "    }\n",
    "\n",
    "train_enc = [preprocess(x) for x in train_data]\n",
    "test_enc = [preprocess(x) for x in test_data]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f292fbf3-0888-47b0-8ac0-6e9a7e6bbc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Convert to TensorFlow Datasets ---\n",
    "def convert_to_tf_dataset(encoded_data):\n",
    "    def gen():\n",
    "        for ex in encoded_data:\n",
    "            yield {\n",
    "                \"input_ids\": ex[\"input_ids\"],\n",
    "                \"attention_mask\": ex[\"attention_mask\"],\n",
    "                \"labels\": ex[\"labels\"],\n",
    "            }\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature={\n",
    "            \"input_ids\": tf.TensorSpec(shape=(MAX_INPUT_LEN,), dtype=tf.int32),\n",
    "            \"attention_mask\": tf.TensorSpec(shape=(MAX_INPUT_LEN,), dtype=tf.int32),\n",
    "            \"labels\": tf.TensorSpec(shape=(MAX_TARGET_LEN,), dtype=tf.int32),\n",
    "        }\n",
    "    ).batch(BATCH_SIZE)\n",
    "\n",
    "train_ds = convert_to_tf_dataset(train_enc)\n",
    "test_ds = convert_to_tf_dataset(test_enc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ea6a255-8cdd-4fa4-bad6-f42405a4d8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# --- Load teacher and student models ---\n",
    "teacher_model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME_TEACHER)\n",
    "student_model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME_STUDENT)\n",
    "\n",
    "# --- Freeze teacher weights ---\n",
    "teacher_model.trainable = False\n",
    "\n",
    "# --- Custom Distillation Loss ---\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, labels, alpha=0.5, temperature=2.0):\n",
    "    hard_loss = loss_object(labels, student_logits)\n",
    "    soft_loss = tf.keras.losses.KLDivergence()(tf.nn.softmax(teacher_logits / temperature),\n",
    "                                                tf.nn.softmax(student_logits / temperature))\n",
    "    return alpha * hard_loss + (1 - alpha) * soft_loss\n",
    "\n",
    "# --- Training Loop ---\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "\n",
    "@tf.function\n",
    "def train_step(batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        student_outputs = student_model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"],\n",
    "            training=True\n",
    "        )\n",
    "        teacher_outputs = teacher_model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"],\n",
    "            training=False\n",
    "        )\n",
    "\n",
    "        loss = distillation_loss(\n",
    "            student_outputs.logits,\n",
    "            teacher_outputs.logits,\n",
    "            batch[\"labels\"]\n",
    "        )\n",
    "\n",
    "    gradients = tape.gradient(loss, student_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, student_model.trainable_variables))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff00e878-e3d9-443e-a794-f620472af865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n",
      "Step 0: Loss = 2.5981\n",
      "Step 2: Loss = 1.7052\n",
      "Step 4: Loss = 2.9662\n",
      "Step 6: Loss = 1.7558\n",
      "\n",
      "Epoch 2/2\n",
      "Step 0: Loss = 2.4881\n",
      "Step 2: Loss = 1.6477\n",
      "Step 4: Loss = 2.9328\n",
      "Step 6: Loss = 1.6988\n"
     ]
    }
   ],
   "source": [
    "# --- Run Training ---\n",
    "EPOCHS = 2\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    for step, batch in enumerate(train_ds):\n",
    "        loss = train_step(batch)\n",
    "        if step % 2 == 0:\n",
    "            print(f\"Step {step}: Loss = {loss.numpy():.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c30df19-f378-489d-b32f-b2aa30dab1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned 145959 of 60506624 weights (~0.24%)\n"
     ]
    }
   ],
   "source": [
    "# --- Manual Pruning (Set small weights to zero) ---\n",
    "def manual_prune(model, threshold=1e-3):\n",
    "    pruned = 0\n",
    "    total = 0\n",
    "    for var in model.trainable_variables:\n",
    "        mask = tf.math.abs(var) < threshold\n",
    "        pruned += tf.reduce_sum(tf.cast(mask, tf.int32)).numpy()\n",
    "        total += tf.size(var).numpy()\n",
    "        var.assign(tf.where(mask, tf.zeros_like(var), var))\n",
    "    print(f\"Pruned {pruned} of {total} weights (~{100 * pruned / total:.2f}%)\")\n",
    "\n",
    "manual_prune(student_model, threshold=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ee21e5f-f256-4940-af3a-3203603703d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_summarizer_model\\saved_model\\1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_summarizer_model\\saved_model\\1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved optimized summarization model to 'tf_summarizer_model/'\n"
     ]
    }
   ],
   "source": [
    "# --- Save as TensorFlow Model ---\n",
    "student_model.save_pretrained(\"tf_summarizer_model\", saved_model=True)\n",
    "print(\"Saved optimized summarization model to 'tf_summarizer_model/'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-transformers)",
   "language": "python",
   "name": "tf-transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
