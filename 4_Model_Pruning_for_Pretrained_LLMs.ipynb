{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU7wdRsH5kFk"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Model Pruning for Pretrained LLMs\n",
    "\n",
    "## 1. Introduction: What is Model Pruning?\n",
    "\n",
    "**Model pruning** is a compression technique that reduces the size and computational complexity of a neural network by removing parameters (typically weights or neurons) that contribute little to the model's overall performance. The key idea is that many parameters in deep neural networks are redundant or underutilized, and can be pruned to create smaller, faster, and more efficient models.\n",
    "\n",
    "Types of pruning:\n",
    "\n",
    "* **Unstructured Pruning**: Removes individual weights regardless of their position in the model. Leads to sparse weight matrices.\n",
    "* **Structured Pruning**: Removes entire units such as neurons, attention heads, or even layers. More hardware-friendly than unstructured pruning.\n",
    "* **Global vs. Layer-wise Pruning**: Global pruning ranks all weights across the model, while layer-wise pruning does this within each layer.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Model Pruning in the Context of Pretrained LLMs\n",
    "\n",
    "### Why Prune LLMs?\n",
    "\n",
    "Pretrained large language models like GPT, BERT, LLaMA, and T5 have hundreds of millions to billions of parameters. Pruning helps in:\n",
    "\n",
    "* **Reducing inference latency**\n",
    "* **Lowering memory footprint**\n",
    "* **Enabling edge deployment**\n",
    "* **Reducing energy usage**\n",
    "* **Serving more models on limited GPU resources**\n",
    "\n",
    "However, pruning LLMs is non-trivial due to:\n",
    "\n",
    "* Their **layered architecture** (transformer blocks),\n",
    "* **Interdependencies** between layers and attention heads,\n",
    "* Sensitivity to performance degradation.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Best Practices in LLM Pruning\n",
    "\n",
    "### Pruning Strategy\n",
    "\n",
    "* **Magnitude-Based Pruning**: Remove weights with the smallest absolute values. Simple but effective.\n",
    "* **Gradient-Based Pruning**: Uses gradients to determine importance, such as SNIP or GraSP.\n",
    "* **Attention Head Pruning**: Identify and remove redundant attention heads (e.g., via attention entropy or importance scores).\n",
    "* **LayerDrop or Layer Pruning**: Remove entire transformer layers carefully, based on layer importance.\n",
    "\n",
    "### Iterative Pruning and Fine-tuning\n",
    "\n",
    "* Avoid one-shot pruning. **Iterative pruning + fine-tuning** is more effective:\n",
    "\n",
    "  1. Prune a small percentage of weights.\n",
    "  2. Fine-tune to recover accuracy.\n",
    "  3. Repeat until target sparsity is reached.\n",
    "\n",
    "### Prune Ratio\n",
    "\n",
    "* Common practice: start with **20–30% sparsity**, then scale up.\n",
    "* 80–90% sparsity is possible, but often with performance trade-offs unless sophisticated techniques are used (e.g., Lottery Ticket Hypothesis or movement pruning).\n",
    "\n",
    "### Metrics to Track\n",
    "\n",
    "* Perplexity (for language generation)\n",
    "* F1, EM, or BLEU scores (for classification or QA)\n",
    "* Inference time (latency, throughput)\n",
    "* FLOPs reduction\n",
    "* Memory usage\n",
    "* Sparsity %\n",
    "\n",
    "---\n",
    "\n",
    "## 4. How to Know if the Model is Properly Pruned\n",
    "\n",
    "A pruned model is considered “proper” when:\n",
    "\n",
    "* **It maintains accuracy within acceptable loss bounds** (e.g., <1% drop in F1 or BLEU).\n",
    "* **It exhibits desired sparsity levels** (e.g., 80% weights pruned).\n",
    "* **It improves inference efficiency** (faster or lower memory use).\n",
    "* **It behaves consistently across datasets and tasks.**\n",
    "\n",
    "### Tools:\n",
    "\n",
    "* **Hugging Face Transformers + Optimum + ONNX Runtime**: Evaluate sparsity and performance.\n",
    "* **TorchPruner, PyTorch Lightning, or SparseML**: Visualize and validate pruning effectiveness.\n",
    "* **NNMeter, DeepSparse, TensorRT, or TVM**: Measure real-world speedups.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Dataset Considerations\n",
    "\n",
    "### Do You Need a Dataset to Prune?\n",
    "\n",
    "Yes: **most pruning strategies require a dataset** for:\n",
    "\n",
    "* **Scoring weight/feature importance**\n",
    "* **Validating loss/performance drops**\n",
    "* **Fine-tuning after pruning**\n",
    "\n",
    "### Choosing the Right Dataset\n",
    "\n",
    "* Use **task-aligned data**: If pruning a QA model, use SQuAD or NaturalQuestions.\n",
    "* For general LLM pruning: Use a **diverse corpus** (e.g., WikiText-103, Pile, OpenWebText).\n",
    "* For language-specific tasks: Choose data in the target language or domain.\n",
    "* You can use a **subset** (even 1-5%) of the training set for iterative pruning/fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Resource Requirements\n",
    "\n",
    "Pruning is less demanding than pretraining, but still non-trivial:\n",
    "\n",
    "* **Memory**: Need to load the full pretrained model and optimizer states.\n",
    "* **Compute**:\n",
    "\n",
    "  * Unstructured pruning: \\~1–2 GPU hours per iteration.\n",
    "  * Structured pruning + fine-tuning: \\~10–100 GPU hours depending on model size and strategy.\n",
    "* **Frameworks**:\n",
    "\n",
    "  * Hugging Face Transformers + PyTorch/TensorFlow\n",
    "  * SparseML, DeepSpeed, and OpenVINO for deployment\n",
    "  * Intel Neural Compressor or Nvidia TensorRT for inference speed-up\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Concerns and Pitfalls\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "* **Loss of performance**: Especially if pruning is too aggressive without fine-tuning.\n",
    "* **Unstructured pruning ≠ speedup**: Sparse weights don’t speed up inference unless the backend supports sparse computation (e.g., DeepSparse).\n",
    "* **Layer collapse**: Removing critical layers can destabilize the model.\n",
    "* **Poor generalization**: Over-pruned models may overfit to narrow domains.\n",
    "* **Deployment compatibility**: Some runtimes do not support sparse models natively.\n",
    "\n",
    "### Things to Avoid\n",
    "\n",
    "* One-shot high-rate pruning\n",
    "* Pruning without validation metrics\n",
    "* Ignoring batch norm / layer norm interactions\n",
    "* Using task-irrelevant data during fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Success Stories and Applications\n",
    "\n",
    "* **OpenAI GPT Pruning (2020)**: Achieved 90% sparsity with <1% loss in accuracy (via movement pruning).\n",
    "* **DistilBERT + Pruning**: Combined distillation and pruning to reduce BERT size by 60% with minimal loss.\n",
    "* **SparseGPT (MIT, 2023)**: A one-shot pruning method for LLMs like OPT and LLaMA, achieving >50% sparsity with minimal accuracy drop.\n",
    "* **DeepSparse + Neural Magic**: Used for production-level sparse inference acceleration, especially in NLP tasks.\n",
    "* **Transformer Head Pruning in BERT**: Showed that 30–40% of attention heads can be pruned without major accuracy loss.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Summary\n",
    "\n",
    "| Aspect           | Recommendation                                                                  |\n",
    "| ---------------- | ------------------------------------------------------------------------------- |\n",
    "| **Goal**         | Reduce size/inference cost with minimal performance loss                        |\n",
    "| **Strategy**     | Iterative pruning + fine-tuning preferred                                       |\n",
    "| **Pruning type** | Structured (e.g., heads/layers) for deployability; unstructured for compression |\n",
    "| **Dataset**      | Required for scoring/fine-tuning; should match task                             |\n",
    "| **Toolkits**     | HuggingFace + SparseML, DeepSparse, Neural Compressor, TorchPruner              |\n",
    "| **Verification** | Track sparsity, accuracy, latency, memory                                       |\n",
    "| **Pitfalls**     | Overpruning, unstructured-only pruning, lack of fine-tuning                     |\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Further Reading & Resources\n",
    "\n",
    "* **Papers**\n",
    "\n",
    "  * [\"The Lottery Ticket Hypothesis\"](https://arxiv.org/abs/1803.03635)\n",
    "  * [\"SparseGPT\"](https://arxiv.org/abs/2301.00774)\n",
    "  * [\"Movement Pruning\"](https://arxiv.org/abs/2005.07683)\n",
    "\n",
    "* **Libraries**\n",
    "\n",
    "  * [SparseML](https://github.com/neuralmagic/sparseml)\n",
    "  * [Hugging Face Optimum](https://huggingface.co/docs/optimum/index)\n",
    "  * [Intel Neural Compressor](https://github.com/intel/neural-compressor)\n",
    "  * [NVIDIA TensorRT](https://developer.nvidia.com/tensorrt)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Pruning an LLM Using TensorFlow\n",
    "- #### SST-2 Dataset\n",
    "\n",
    "- #### Constraints:\n",
    "> - ✅ Runs on CPU\n",
    "> - ✅ Uses TF-compatible Hugging Face models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Preprocess SST-2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"sentence\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(tokenize_function, batched=True)\n",
    "tokenized.set_format(type='tensorflow', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tf_dataset(tokenized_dataset):\n",
    "    return tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            \"input_ids\": tokenized_dataset[\"input_ids\"],\n",
    "            \"attention_mask\": tokenized_dataset[\"attention_mask\"]\n",
    "        },\n",
    "        tokenized_dataset[\"label\"]\n",
    "    )).batch(BATCH_SIZE)\n",
    "\n",
    "train_ds = convert_to_tf_dataset(tokenized['train'].shuffle(1000).select(range(5000)))\n",
    "val_ds = convert_to_tf_dataset(tokenized['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = TFAutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Freeze base model to speed up training\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"input_ids\": tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\"),\n",
    "    \"attention_mask\": tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\n",
    "}\n",
    "\n",
    "outputs = base_model(inputs)[0][:, 0, :]  # [CLS] token\n",
    "outputs = tf.keras.layers.Dense(64, activation='relu', name=\"dense_1\")(outputs)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid', name=\"classifier\")(outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training baseline model...\")\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=2)\n",
    "### note: this can take up to 10 min per epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_loss, baseline_acc = model.evaluate(val_ds)\n",
    "print(f\"\\nBaseline Accuracy: {baseline_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = next(iter(val_ds))\n",
    "start_time = time.time()\n",
    "_ = model.predict(sample_batch[0])\n",
    "print(f\"Baseline inference time: {time.time() - start_time:.4f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_weights_by_magnitude(model, sparsity=0.5):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Dense):\n",
    "            weights, biases = layer.get_weights()\n",
    "            threshold = np.percentile(np.abs(weights), sparsity * 100)\n",
    "            pruned_weights = np.where(np.abs(weights) < threshold, 0, weights)\n",
    "            layer.set_weights([pruned_weights, biases])\n",
    "    return model\n",
    "\n",
    "# Apply pruning\n",
    "\n",
    "print(\"\\nPruning weights manually...\")\n",
    "pruned_model = prune_weights_by_magnitude(model, sparsity=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_loss, pruned_acc = pruned_model.evaluate(val_ds)\n",
    "print(f\"\\nPruned Accuracy: {pruned_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure inference time after pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "_ = pruned_model.predict(sample_batch[0])\n",
    "print(f\"Pruned inference time: {time.time() - start_time:.4f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Pruned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model.save(\"pruned_model\")\n",
    "print(\"\\nPruned model saved as 'pruned_model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
