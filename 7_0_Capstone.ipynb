{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bba7bbd-1031-4333-ad97-0c729a861385",
   "metadata": {},
   "source": [
    "# Hackathon: Optimize and Deploy Text Summarization Model\n",
    "\n",
    "---\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "- Optimize a transformer-based text summarization model for efficient deployment in a resource-constrained environment to simulate a production scenario.\n",
    "- Use **knowledge distillation** and **manual pruning** techniques to reduce model size and inference latency without significantly compromising summarization quality.\n",
    "- Use a subset of the CNN/DailyMail dataset and fine-tune a student model (`t5-small`).\n",
    "- Apply structured weight pruning to further compress the model.\n",
    "- The final model is saved in TensorFlow format, suitable for containerized inference deployment.\n",
    "\n",
    "---\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. **Subsampling** you need to subsample the original CNN/DailyMail dataset to adjust it to the small compute resources\n",
    "2. **Distillation**: Train a compact student model to mimic the output behavior of a larger teacher model using both hard labels and soft logits.\n",
    "3. **Pruning**: Apply magnitude-based weight pruning manually to remove unimportant weights from the trained student model.\n",
    "4. **TensorFlow Only**: All components—models, training, pruning, and saving—must use TensorFlow (no PyTorch).\n",
    "5. **No Quantization**: Skip any quantization techniques in this notebook.\n",
    "6. **Deployment Ready**: Save the optimized model using TensorFlow’s `SavedModel` format for later use in Docker and ECS environments.\n",
    "7. **Bonus**: Experiment with subsample size, batch size and number of epochs.\n",
    "\n",
    "---\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "* The dataset is a pre-subsampled version of CNN/DailyMail (\\~200 training and \\~50 test examples or less) and is available locally in JSON format.\n",
    "* The selected models (`t5-base` and `t5-small`) are fully supported by Hugging Face's TensorFlow APIs.\n",
    "* Training is conducted on a CPU-only machine (e.g., AWS `t2.large`), requiring careful management of memory and runtime.\n",
    "* Only basic training (2 epochs) is required to demonstrate optimization strategies, not to reach state-of-the-art performance.\n",
    "* Final output will be integrated into a containerized microservice, so model size and inference efficiency are critical.\n",
    "\n",
    "---\n",
    "\n",
    "Here's a concise yet informative **description of the CNN/DailyMail dataset** that you can include in your notebook or project documentation:\n",
    "\n",
    "---\n",
    "\n",
    "## CNN/DailyMail Dataset Description\n",
    "\n",
    "The **CNN/DailyMail** dataset is a large-scale benchmark for **abstractive text summarization**, widely used in natural language processing research. It consists of news articles from CNN and the Daily Mail, paired with human-written summaries, often referred to as *highlights*.\n",
    "\n",
    "Each example in the dataset includes:\n",
    "\n",
    "* **`article`**: The full body of a news story (approx. 300–800 words).\n",
    "* **`highlights`**: A concise, human-curated summary capturing the main points (1–3 sentences).\n",
    "\n",
    "### Dataset Versions\n",
    "\n",
    "* The most commonly used version is **\"3.0.0\"**, which filters out anonymized entity tags and provides clean text.\n",
    "* The dataset is split into:\n",
    "\n",
    "  * `train`: \\~287,000 examples\n",
    "  * `validation`: \\~13,000 examples\n",
    "  * `test`: \\~11,000 examples\n",
    "\n",
    "### Use Case in This Project\n",
    "\n",
    "In this project, we use a **subsampled version** of the dataset (e.g., 200 train, 50 test examples) to simulate summarization model training in a resource-constrained environment. The summaries serve as target outputs for fine-tuning a student model using knowledge distillation techniques.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aed08fe-a249-45af-9a41-14f0c048b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Imports ---\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# --- Load and prepare data ---\n",
    "with open(\"subsampled_cnn_dailymail/train_sample.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"subsampled_cnn_dailymail/test_sample.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-transformers)",
   "language": "python",
   "name": "tf-transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
