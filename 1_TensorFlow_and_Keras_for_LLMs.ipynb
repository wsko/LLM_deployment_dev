{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMslwSjK71vw"
   },
   "source": [
    "# TensorFlow and Keras for LLMs: From Prototyping to Deployment\n",
    "\n",
    "---\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. **Introduction to TensorFlow and Keras**\n",
    "2. **TensorFlow and Keras Essentials: A Refresher**\n",
    "\n",
    "   * Tensors and Data Types\n",
    "   * Data Loading and Preprocessing\n",
    "   * The Significance of TensorFlow and Keras in ML\n",
    "3. **Building Neural Networks in Keras**\n",
    "\n",
    "   * Sequential API\n",
    "   * Functional API\n",
    "4. **LLMs in the TensorFlow Ecosystem**\n",
    "\n",
    "   * Overview of LLMs\n",
    "   * Hugging Face Transformers with TensorFlow\n",
    "   * Other Compatible LLM Libraries\n",
    "5. **Model Importing and Inference**\n",
    "\n",
    "   * Importing Pretrained LLMs with TensorFlow\n",
    "   * Running Inference Locally\n",
    "   * Hardware Considerations\n",
    "6. **Prototyping to Deployment**\n",
    "\n",
    "   * TF SavedModel Format\n",
    "   * TensorFlow Serving / TF Lite / TF.js\n",
    "   * Integration into Applications (e.g., Flask, FastAPI)\n",
    "7. **References and Resources**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGwp2pxa79Uq"
   },
   "source": [
    "\n",
    "## 1. Introduction to TensorFlow and Keras\n",
    "\n",
    "* TensorFlow (TF): end-to-end open-source platform for machine learning developed by Google.\n",
    "* TF provides tools for model building, training, serving, and deployment.\n",
    "* Keras is its high-level API for building and training deep learning models with a user-friendly interface.\n",
    "\n",
    "##### TensorFlow and Keras together provide:\n",
    "\n",
    "* Scalable execution on CPUs, GPUs, TPUs\n",
    "* Flexibility (eager and graph modes)\n",
    "* Interoperability with the broader Python ML ecosystem\n",
    "* Integration with powerful tools like TensorBoard, TFHub, and TFX\n",
    "\n",
    "##### Note:\n",
    "- PyTorch dominates open source large LLM space\n",
    "- TensorFlow still remains a robust option for enterprise-grade ML/LLM solutions.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q78OuJzW8JA1"
   },
   "source": [
    "\n",
    "## 2. TensorFlow and Keras Essentials: A Refresher\n",
    "\n",
    "### Tensors and Data Types\n",
    "\n",
    "- Tensors are the core data structure in TensorFlow\n",
    "- Tensors are multi-dimensional arrays with a uniform data type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQCYn-5wupXx"
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jHU-UsHupXx"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWL-lcO_8M0Y"
   },
   "outputs": [],
   "source": [
    "a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "print(a)\n",
    "print('---')\n",
    "print(a.shape)      # (2, 2)\n",
    "print('---')\n",
    "print(a.dtype)      # tf.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llYHfz8D8WcX"
   },
   "source": [
    "Common dtypes:\n",
    "\n",
    "* `tf.float32`, `tf.float64`\n",
    "* `tf.int32`, `tf.int64`\n",
    "* `tf.string`, `tf.bool`\n",
    "\n",
    "### Data Loading and Preprocessing\n",
    "\n",
    "Data ingestion in TensorFlow is handled via the `tf.data` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ddu2_12upXy"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(\".\\data\\hawking.txt\")\n",
    "#The dataset now contains a sequence of tf.string tensors, where each tensor is one line from your text file.\n",
    "dataset = dataset.map(lambda x: tf.strings.split(x))\n",
    "# The dataset now contains a sequence of tf.Tensor objects, where each tensor is a list of words (or \"tokens\") from the original line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kp4ayzZtupXz"
   },
   "outputs": [],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preview Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first few lines\n",
    "for i, line in enumerate(dataset.take(5)):\n",
    "    tokens = tf.reshape(line, [-1]).numpy()  # ensure flat array\n",
    "    print(f\"Line {i+1} tokens:\", [token.decode(\"utf-8\") for token in tokens])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pq4N8QkE8tvW"
   },
   "source": [
    "\n",
    "It supports streaming, batching, caching, shuffling, and prefetching for performance.\n",
    "\n",
    "For tokenized datasets (common in LLM workflows), integration with Hugging Face Datasets is common using interoperability bridges.\n",
    "\n",
    "### Why TensorFlow/Keras?\n",
    "\n",
    "* **Production-readiness** with TFX and TensorFlow Serving\n",
    "* **Tooling**: TensorBoard, Model Optimization Toolkit, TF Lite\n",
    "* **Scalability**: GPU/TPU acceleration with XLA\n",
    "* **Compatibility**: Hugging Face, ONNX, JAX interop\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJV3D8tg8ydo"
   },
   "source": [
    "\n",
    "## 3. Building Neural Networks in Keras\n",
    "\n",
    "### What is a Neural Network?\n",
    "\n",
    "### Sequential API\n",
    "\n",
    "Ideal for linear stacks of layers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANxb3RgG84_j"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(784,)), # Explicit Input layer\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTzwTnuV9AwG"
   },
   "source": [
    "### Functional API\n",
    "\n",
    "Supports complex architectures (multiple inputs/outputs, residuals):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMhxwzAb9KRr"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, concatenate\n",
    "\n",
    "input1 = Input(shape=(64,))\n",
    "input2 = Input(shape=(32,))\n",
    "x1 = Dense(128, activation='relu')(input1)\n",
    "x2 = Dense(64, activation='relu')(input2)\n",
    "combined = concatenate([x1, x2])\n",
    "output = Dense(1, activation='sigmoid')(combined)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoN8rmF29TrL"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 4. LLMs in the TensorFlow Ecosystem\n",
    "\n",
    "### Overview of LLMs\n",
    "\n",
    "LLMs (e.g., GPT, BERT, T5) are transformer-based models trained on massive corpora. Tasks include:\n",
    "\n",
    "* Text classification\n",
    "* Named entity recognition (NER)\n",
    "* Text summarization\n",
    "* Question answering\n",
    "* Code generation\n",
    "\n",
    "### Hugging Face Transformers with TensorFlow\n",
    "\n",
    "Most Hugging Face models support PyTorch and TensorFlow backends:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihu21Gal9bvs"
   },
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARbPO4BFASsl"
   },
   "source": [
    "\n",
    "### Other Compatible Libraries\n",
    "\n",
    "* **TFHub**: Pretrained models for text and vision\n",
    "* **KerasNLP**: Native Keras tools for tokenization, embedding, transformer layers\n",
    "* **T5X / Flax models**: Often require conversion but increasingly interoperable\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Model Importing and Inference\n",
    "\n",
    "### Importing Pretrained Models\n",
    "\n",
    ">- DistilBERT for question answering works by encoding both the question and the context text together into token embeddings.\n",
    ">- The model then predicts two positions in the combined input: the start and end tokens of the answer span within the context, allowing it to extract the most relevant answer directly from the passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMO7MBn69k1-"
   },
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForQuestionAnswering\n",
    "\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yjBHexr9swG"
   },
   "source": [
    "### Running Locally for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3GZ5Ja-9tRF"
   },
   "outputs": [],
   "source": [
    "question = \"What is TensorFlow?\"\n",
    "context = \"TensorFlow is an end-to-end open-source platform for machine learning developed by Google.\"\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"tf\")\n",
    "inputs.pop(\"token_type_ids\", None)  # if model doesn't use them\n",
    "\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3RbA4VEupX1"
   },
   "outputs": [],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJIcuqHDupX1"
   },
   "outputs": [],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get start and end logits\n",
    "start_logits = outputs.start_logits  # (1, seq_len)\n",
    "end_logits = outputs.end_logits      # (1, seq_len)\n",
    "\n",
    "# 2. Find the most likely start and end token positions\n",
    "start_index = tf.argmax(start_logits, axis=1).numpy()[0]\n",
    "end_index = tf.argmax(end_logits, axis=1).numpy()[0]\n",
    "\n",
    "# 3. Get input tokens IDs\n",
    "input_ids = inputs[\"input_ids\"].numpy()[0]\n",
    "\n",
    "# 4. Extract answer tokens IDs from input\n",
    "answer_ids = input_ids[start_index : end_index + 1]\n",
    "\n",
    "# 5. Decode tokens to text\n",
    "answer = tokenizer.decode(answer_ids)\n",
    "\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Vl1Omyu-AvT"
   },
   "source": [
    "\n",
    "Inference can be optimized using:\n",
    "\n",
    "* **XLA compilation**\n",
    "* **Mixed-precision** (FP16) for GPU\n",
    "* **SavedModel export** for efficient serving\n",
    "\n",
    "### Hardware Considerations\n",
    "\n",
    "* CPU inference possible but slow\n",
    "* GPU (CUDA) or Apple Silicon acceleration preferred\n",
    "* TPU use supported in GCP or Colab\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKCQxUze-sKj"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 6. Prototyping to Deployment\n",
    "\n",
    "#### Saving the Pretrained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hB_kOtW-3wA"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./saved_model\")\n",
    "\n",
    "### Or convert to TensorFlow format:\n",
    "\n",
    "model.save(\"./tf_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1Dm8cdq4Tw1"
   },
   "source": [
    "\n",
    "### Serving Options\n",
    "\n",
    "* **TensorFlow Serving** for REST/gRPC interfaces\n",
    "* **TFLite** for mobile devices\n",
    "* **TF.js** for browser-based inference\n",
    "* **ONNX** export (via `transformers.onnx`) for broad compatibility\n",
    "\n",
    "### Integration into Applications\n",
    "\n",
    "Using Flask or FastAPI for REST APIs:\n",
    "\n",
    "```python\n",
    "from flask import Flask, request\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model(\"tf_model\")\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    data = request.json\n",
    "    tokens = tokenizer(data[\"text\"], return_tensors=\"tf\")\n",
    "    output = model(**tokens)\n",
    "    return output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. References and Resources\n",
    "\n",
    "* [TensorFlow Official Site](https://www.tensorflow.org/)\n",
    "* [Keras Documentation](https://keras.io/)\n",
    "* [Hugging Face Transformers](https://huggingface.co/transformers/)\n",
    "* [TensorFlow Model Garden](https://github.com/tensorflow/models)\n",
    "* [KerasNLP](https://keras.io/keras_nlp/)\n",
    "* [TF Lite](https://www.tensorflow.org/lite)\n",
    "* [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)\n",
    "* [Hugging Face + TensorFlow Guide](https://huggingface.co/docs/transformers/index)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xHTEHx-4SiP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkG4Mvmpux_O"
   },
   "source": [
    "## Lab: Introduction to TensorFlow and Keras for LLMs\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "  * Understand basic TensorFlow data types and operations.\n",
    "  * Load and preprocess text data for LLMs.\n",
    "  * Utilize Hugging Face Transformers with TensorFlow/Keras for LLM tasks.\n",
    "  * Generate BERT embeddings for text.\n",
    "  * Perform text classification using a pre-trained LLM.\n",
    "  * Experiment with basic generative LLMs.\n",
    "  * (Optional Advanced) Understand the concepts and perform a lightweight fine-tuning of an LLM.\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD8ikgrjux_S"
   },
   "source": [
    "**TensorFlow Basics:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cl6jn_Uaux_S"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvL9iQ5Eux_T"
   },
   "outputs": [],
   "source": [
    "# Create a constant tensor\n",
    "my_tensor = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
    "print(\"My Tensor:\\n\", my_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0VgyvpFux_T"
   },
   "outputs": [],
   "source": [
    "# Basic operation\n",
    "added_tensor = my_tensor + 5\n",
    "print(\"Added Tensor:\\n\", added_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CJaChBcux_U"
   },
   "outputs": [],
   "source": [
    "# Convert to NumPy\n",
    "numpy_array = my_tensor.numpy()\n",
    "print(\"NumPy Array:\\n\", numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIYAYnqzux_U"
   },
   "source": [
    "**Tokenization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PY8TGIJNux_U"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "sentences = [\"Hello, how are you today?\", \"I love deploying LLMs!\"]\n",
    "tokenized_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "\n",
    "print(\"Input IDs:\\n\", tokenized_inputs[\"input_ids\"])\n",
    "print(\"Attention Mask:\\n\", tokenized_inputs[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-k9xxqhbux_V"
   },
   "outputs": [],
   "source": [
    "# token vocabulary\n",
    "print(\"Tokenizer vocab (sample):\")\n",
    "for token, idx in list(tokenizer.vocab.items())[:20]:\n",
    "    print(f\"'{token}': {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hgoDWZVux_V"
   },
   "source": [
    "**BERT Embeddings:**\n",
    "\n",
    "> - Load a DistilBERT tokenizer and model to convert a batch of input sentences into token embeddings. It tokenizes the sentences with padding and truncation, then passes them through the model to obtain the last hidden states\n",
    "> - Last hidden state = encoded meaning of each token (word) given all other tokens in the sentence\n",
    "> - To get a simple summary for each whole sentence, it picks the numbers from the `CLS` special token. These numbers can then be used to compare sentences or feed into other programs that understand this kind of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Mft9Tyzux_V"
   },
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = TFAutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "sentences = [\"Hello, how are you today?\", \"I love deploying LLMs!\"]\n",
    "tokenized_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "\n",
    "# Get embeddings\n",
    "outputs = model(tokenized_inputs)\n",
    "last_hidden_state = outputs.last_hidden_state  # shape: (batch_size, seq_len, hidden_size)\n",
    "\n",
    "# Use first token's embedding as pooled output (DistilBERT has no pooler_output)\n",
    "pooled_output = last_hidden_state[:, 0, :]  # shape: (batch_size, hidden_size)\n",
    "\n",
    "print(\"Last Hidden State shape:\", last_hidden_state.shape)\n",
    "print(\"Pooled Output shape:\", pooled_output.shape)\n",
    "print(\"First sentence embedding (pooled, first 5 dims):\", pooled_output[0, :5].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute similarity between sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHkefyi1ux_V"
   },
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = TFAutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\n",
    "    \"I love machine learning.\",\n",
    "    \"Artificial intelligence is fascinating.\",\n",
    "    \"I enjoy deploying LLMs!\",\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"Green tea has a soothing effect.\",\n",
    "    \"I hate machine learning.\"\n",
    "]\n",
    "\n",
    "# Tokenize with padding and truncation\n",
    "tokenized_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "\n",
    "# Get model outputs\n",
    "outputs = model(tokenized_inputs)\n",
    "last_hidden_state = outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "# Mean pooling: average over the token embeddings for each sentence\n",
    "pooled_embeddings = tf.reduce_mean(last_hidden_state, axis=1)  # (batch_size, hidden_size)\n",
    "\n",
    "# Normalize embeddings to unit vectors\n",
    "normalized_embeddings = tf.math.l2_normalize(pooled_embeddings, axis=1)\n",
    "\n",
    "# Compute cosine similarity matrix between all pairs\n",
    "cosine_sim_matrix = tf.matmul(normalized_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "# Convert to numpy for easy printing\n",
    "cosine_sim_matrix_np = cosine_sim_matrix.numpy()\n",
    "\n",
    "print(\"Cosine similarity matrix:\")\n",
    "print(np.round(cosine_sim_matrix_np, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB4NjE5Nux_W"
   },
   "source": [
    "**LLM Classification (Tiny Example):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5JjvPFqux_W"
   },
   "outputs": [],
   "source": [
    "# import tf_keras as keras\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tgef-3NDux_W"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "imdb = pd.read_csv(\".\\data\\imdb_reviews.csv\")\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NspaClHux_W"
   },
   "outputs": [],
   "source": [
    "# Fake small dataset\n",
    "#texts = [\"This movie was great!\", \"Terrible film.\", \"Neutral review.\", \"Absolutely amazing!\"]\n",
    "#labels = [1, 0, 1, 1] # 1 for positive, 0 for negative/neutral\n",
    "\n",
    "texts = list(imdb['review'].values)[:20]\n",
    "labels = list(imdb['sentiment'].values)[:20]\n",
    "\n",
    "# Tokenize\n",
    "tokenized_data = tokenizer(texts, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "tf_labels = tf.constant(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGTgsh9Oux_W"
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cy2srDulux_W"
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "classifier_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "\n",
    "# Compile model\n",
    "classifier_model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTmscnoyux_X"
   },
   "outputs": [],
   "source": [
    "# Train for a very small number of epochs with a tiny batch size\n",
    "history = classifier_model.fit(\n",
    "    x=dict(tokenized_data),\n",
    "    y=tf_labels,\n",
    "    epochs=5, # Very few epochs\n",
    "    batch_size=2, # Tiny batch size\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gp0BjAK-ux_X"
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "#new_text = [\"This is a fantastic example!\"]\n",
    "new_text = imdb['review'][20]\n",
    "new_tokenized_input = tokenizer(new_text, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "logits = classifier_model.predict(new_tokenized_input).logits\n",
    "predicted_class_id = tf.argmax(logits, axis=1).numpy()[0]\n",
    "print(f\"Predicted class for '{new_text[0]}': {predicted_class_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KgVkVFEux_X"
   },
   "outputs": [],
   "source": [
    "new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbUBf_bsux_X"
   },
   "source": [
    "**Generative LLM:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gif2gYwsux_X"
   },
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Use matching model and tokenizer\n",
    "model_name = \"gpt2\"  # You can change this to \"distilgpt2\" for a smaller model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "generator_model = TFAutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njk8JTKe0o8S"
   },
   "source": [
    "**Generate text**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbP7-A6g0oTN"
   },
   "outputs": [],
   "source": [
    "# Input prompt\n",
    "input_text = \"The quick brown fox jumps over the\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"tf\")\n",
    "\n",
    "# Generate output\n",
    "generated_output = generator_model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    num_return_sequences=1,\n",
    "    top_k=50,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "# Decode and print\n",
    "decoded_output = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\\n\", decoded_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixr6gzLwux_X"
   },
   "source": [
    "#### Experiment with Temperature and top_p\n",
    "- try high temperature (> 1.5) and gradually decrease top_p from 1 to 0.95\n",
    "- try a few different prompts\n",
    "#### Try a smaller generative LLM\n",
    "- replace gpt2 (117M parameters) with distilgpt2 (82M parameters). Does this affect model latency (make it generate faster?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yq-IS4cJux_X"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
