{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_bQciEu4ojL"
   },
   "source": [
    "# **Model Quantization for Pretrained Large Language Models (LLMs)**\n",
    "\n",
    "## **1. Introduction to Model Quantization**\n",
    "\n",
    "**Model quantization** is a technique used to reduce the memory footprint and computational cost of machine learning models by representing parameters with fewer bits. For example, instead of using 32-bit floating point numbers (FP32), weights can be stored as 8-bit integers (INT8) or even lower. This has proven essential for:\n",
    "\n",
    "* **Deploying models to resource-constrained environments** (e.g., mobile, edge devices)\n",
    "* **Speeding up inference** on CPUs or GPUs with lower precision support\n",
    "* **Reducing energy usage and costs in data centers**\n",
    "\n",
    "### Key Quantization Types:\n",
    "\n",
    "| Type                                   | Description                                                                                 |\n",
    "| -------------------------------------- | ------------------------------------------------------------------------------------------- |\n",
    "| **Post-Training Quantization (PTQ)**   | Quantize a trained model without retraining it. Fast and easy, but may degrade accuracy.    |\n",
    "| **Quantization-Aware Training (QAT)**  | Simulates quantization during training, leading to better accuracy but requires retraining. |\n",
    "| **Dynamic Quantization**               | Activates quantization only during inference, suitable for RNNs and LLMs.                   |\n",
    "| **Weight-only Quantization**           | Only the model weights are quantized; activations remain in higher precision.               |\n",
    "| **Activation and Weight Quantization** | Quantizes both, requiring careful calibration or training.                                  |\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Quantization for Pretrained LLMs**\n",
    "\n",
    "### Why Quantize LLMs?\n",
    "\n",
    "Large Language Models such as GPT, LLaMA, Falcon, and Mistral typically require:\n",
    "\n",
    "* **Billions of parameters**\n",
    "* **Multiple GBs of memory** (e.g., 30B model takes >60 GB in FP16)\n",
    "* **Powerful accelerators (GPUs/TPUs)**\n",
    "\n",
    "Quantization reduces this burden, enabling:\n",
    "\n",
    "* Deployment on **consumer-grade GPUs** (e.g., 8‚Äì16 GB VRAM)\n",
    "* Running models on **CPUs or edge devices**\n",
    "* Faster inference with **minimal accuracy loss**\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Best Practices in Quantizing LLMs**\n",
    "\n",
    "### ‚úÖ Choose the Right Quantization Format:\n",
    "\n",
    "* **INT8**: Best tradeoff between performance and accuracy (common in production).\n",
    "* **INT4 (e.g., QLoRA, GPTQ)**: Smaller size, still competitive accuracy; ideal for inference.\n",
    "* **NF4 (Normalized Float 4)**: Specialized format used in fine-tuning-aware quantization (e.g., QLoRA).\n",
    "* **FP8 or bfloat16**: Emerging formats supported by NVIDIA and Google hardware.\n",
    "\n",
    "### ‚úÖ Use Purpose-Built Libraries:\n",
    "\n",
    "| Tool / Library                         | Description                                                          |\n",
    "| -------------------------------------- | -------------------------------------------------------------------- |\n",
    "| **GPTQ**                               | Fast post-training quantization, optimized for LLMs (INT4/INT3).     |\n",
    "| **bitsandbytes**                       | Offers 8-bit optimizers and quantized linear layers (used in QLoRA). |\n",
    "| **LLM.int8()**                         | HuggingFace integration for INT8 quantization.                       |\n",
    "| **Intel Neural Compressor / OpenVINO** | INT8 quantization and optimization for CPUs.                         |\n",
    "| **NVIDIA TensorRT-LLM**                | For highly optimized inference on NVIDIA GPUs.                       |\n",
    "\n",
    "### ‚úÖ Calibrate Properly:\n",
    "\n",
    "* For PTQ, use a small representative **calibration dataset** (100‚Äì1,000 examples).\n",
    "* Choose diverse prompts if you‚Äôre deploying for general-purpose inference.\n",
    "\n",
    "### ‚úÖ Quantize Layers Selectively:\n",
    "\n",
    "* Some layers (e.g., attention heads, embeddings) are more sensitive.\n",
    "* Use mixed precision: keep sensitive layers in FP16 or FP32.\n",
    "\n",
    "### ‚úÖ Evaluate Thoroughly:\n",
    "\n",
    "* Run **perplexity**, **BLEU/ROUGE**, or **task-specific accuracy** before/after quantization.\n",
    "* Evaluate latency, memory, and throughput.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Key Concerns and Challenges**\n",
    "\n",
    "### ‚ùó Accuracy Degradation:\n",
    "\n",
    "* Aggressive quantization (e.g., INT3 or below) may lead to performance drops, especially on reasoning tasks.\n",
    "\n",
    "### ‚ùó Hardware Compatibility:\n",
    "\n",
    "* Some quantized formats require special hardware (e.g., INT4 may not run efficiently on older GPUs or CPUs).\n",
    "\n",
    "### ‚ùó Lack of Fine-Tuning:\n",
    "\n",
    "* PTQ doesn't retrain the model. Some use cases may need QAT or fine-tuning with QLoRA.\n",
    "\n",
    "### ‚ùó Precision Accumulation:\n",
    "\n",
    "* Integer math can accumulate errors over layers‚Äîquantization-aware models account for this.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Success Stories**\n",
    "\n",
    "### üîπ **QLoRA (Quantized Low-Rank Adaptation)**\n",
    "\n",
    "* HuggingFace + Tim Dettmers (2023)\n",
    "* Used NF4 quantization + LoRA for fine-tuning\n",
    "* Enabled fine-tuning 65B models on a single 48GB GPU\n",
    "* Comparable accuracy to full-precision fine-tuned models\n",
    "\n",
    "### üîπ **GPTQ**\n",
    "\n",
    "* Open-source quantization tool\n",
    "* Allows loading 30B+ models on 16GB VRAM with <1% accuracy loss\n",
    "* Hugely popular in the LLM community for deploying LLaMA, Falcon, etc.\n",
    "\n",
    "### üîπ **Intel‚Äôs INT8 BERT Optimizations**\n",
    "\n",
    "* Showed 2x‚Äì4x performance gain on CPUs using INT8 quantized BERT with <1% drop in accuracy\n",
    "* Used in real-world document processing pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## **6. How to Tell If a Model Is Quantized**\n",
    "\n",
    "* **File size**: Quantized models are significantly smaller (e.g., 4-bit model is \\~1/8 size of FP32).\n",
    "* **Configuration files**: HuggingFace models include `quantization_config.json` or show quantization format in model card.\n",
    "* **Inference logs**: Quantized inference often uses custom kernels (e.g., `bitsandbytes`, `AutoGPTQ`, `TRT-LLM`) which appear in logs.\n",
    "* **Model loading APIs**: Example:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7B-GPTQ\", device_map=\"auto\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Do You Need a Dataset?**\n",
    "\n",
    "* **Post-Training Quantization**: Needs only a small calibration set (100‚Äì1000 diverse examples).\n",
    "* **QAT or QLoRA**: Needs training data for fine-tuning; use domain-specific data if targeting a specific use case.\n",
    "* **Inference-only**: Pre-quantized models can be used out-of-the-box.\n",
    "\n",
    "### Dataset Tips:\n",
    "\n",
    "* Include **representative prompts** for your use case\n",
    "* Cover **edge cases**, if high reliability is needed\n",
    "* Mix different **linguistic patterns** (instructions, QA, chat, documents)\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Resource Requirements**\n",
    "- examples for 30B parameters\n",
    "\n",
    "| Quantization Type                 | GPU RAM Needed                | CPU Friendly?             | Notes                             |\n",
    "| --------------------------------- | ----------------------------- | ------------------------- | --------------------------------- |\n",
    "| **FP32**                          | Very High (‚â•60 GB for 30B)    | No                        | Full accuracy                     |\n",
    "| **FP16/BF16**                     | Moderate (30‚Äì45 GB for 30B)   | No                        | Good for training                 |\n",
    "| **INT8**                          | Moderate (\\~12‚Äì16 GB for 30B) | Yes (with AVX512 or VNNI) | Widely supported                  |\n",
    "| **INT4 (GPTQ, QLoRA)**            | Low (\\~6‚Äì8 GB for 30B)        | Partial                   | Best for low-end GPUs             |\n",
    "| **QAT**                           | Moderate to high              | No                        | Requires retraining resources     |\n",
    "| **bitsandbytes (8-bit training)** | 16 GB+                        | No                        | Efficient training with some loss |\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Summary and Recommendations**\n",
    "\n",
    "| Scenario                                 | Recommendation                                  |\n",
    "| ---------------------------------------- | ----------------------------------------------- |\n",
    "| Just want fast inference on LLMs         | Use GPTQ-quantized models (4-bit)               |\n",
    "| Low-VRAM GPU but want to fine-tune       | Use QLoRA + NF4 (with bitsandbytes)             |\n",
    "| Need CPU inference                       | Use INT8 models with OpenVINO or ONNX Runtime   |\n",
    "| Max performance, no concern for accuracy | Use aggressive quantization (INT3/INT4)         |\n",
    "| High accuracy required                   | Use mixed precision or QAT with selected layers |\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Resources and Tools**\n",
    "\n",
    "* **Libraries**:\n",
    "\n",
    "  * [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)\n",
    "  * [GPTQ](https://github.com/IST-DASLab/gptq)\n",
    "  * [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)\n",
    "  * [OpenVINO](https://github.com/openvinotoolkit/openvino)\n",
    "  * [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)\n",
    "\n",
    "* **Tutorials & Docs**:\n",
    "\n",
    "  * HuggingFace: [Quantization Overview](https://huggingface.co/docs/transformers/perf_train_gpu_one#8-bit-quantization)\n",
    "  * Intel: [Neural Compressor](https://github.com/intel/neural-compressor)\n",
    "  * NVIDIA: [TensorRT-LLM Examples](https://github.com/NVIDIA/TensorRT-LLM)\n",
    "\n",
    "* **Communities**:\n",
    "\n",
    "  * [HuggingFace Forums](https://discuss.huggingface.co/)\n",
    "  * [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/)\n",
    "  * Discord servers for HuggingFace, BitsAndBytes, and GPTQ\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 2: Quantizing a Pre-trained LLM using TensorFlow \n",
    "- ### IMDB Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"TF version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess IMDB Dataset\n",
    "\n",
    "> - The IMDB dataset (Internet Movie Database) is a popular benchmark dataset for binary sentiment classification.\n",
    "> - Contains 50,000 movie reviews from IMDb.\n",
    "> - Each review is a raw English sentence or paragraph.\n",
    "> - Each review is labeled: 0 = negative, 1 = positive\n",
    "> - IMDG dataset is commonly used for training and testing text classification models, especially for sentiment analysis tasks.\n",
    "> - Dataset Breakdown: train: 25,000 labeled reviews, test: 25,000 labeled reviews\n",
    "\n",
    "\n",
    "#### In this section:\n",
    "- Load and tokenize the dataset\n",
    "- Vectorize the data by converting it to a numerical array\n",
    "- Use vectorized text as feature set for training a model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading IMDB dataset...\")\n",
    "(train_ds, test_ds), ds_info = tfds.load(\n",
    "    'imdb_reviews',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use a TextVectorization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "SEQUENCE_LENGTH = 256\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adapt vectorizer to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "train_ds = train_ds.map(vectorize_text).cache().shuffle(10000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.map(vectorize_text).batch(32).cache().prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Small Transformer-Based Model\n",
    "\n",
    "- We will build a custom simple Transformer-based binary text classifier\n",
    "- Model Overview:\n",
    "    - Input: A sequence of token integers (e.g., tokenized text, length = SEQUENCE_LENGTH).\n",
    "    - Embedding Layer: Maps each token to a 64-dimensional vector.\n",
    "    - Layer Normalization: Normalizes the embeddings to stabilize training.\n",
    "    - Multi-Head Attention: Lets the model focus on different parts of the sequence when processing each word (2 attention heads).\n",
    "    - Global Average Pooling: Collapses the sequence into a single vector by averaging token representations.\n",
    "    - Dense Layer: Applies a fully connected layer with ReLU to capture nonlinear patterns.\n",
    "    - Dropout: Randomly zeroes some values to prevent overfitting.\n",
    "    - Output Layer: A single neuron with sigmoid activation, producing a probability for binary classification (e.g., positive vs negative sentiment).\n",
    "- Where this can be used:\n",
    "    - Text classification tasks like sentiment analysis, spam detection, or toxic comment classification, using a lightweight Transformer-style architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "print(\"\\nBuilding model...\")\n",
    "\n",
    "embedding_dim = 64\n",
    "num_heads = 2\n",
    "dff = 64\n",
    "\n",
    "inputs = layers.Input(shape=(SEQUENCE_LENGTH,))\n",
    "x = layers.Embedding(VOCAB_SIZE, embedding_dim)(inputs)\n",
    "x = layers.LayerNormalization()(x)\n",
    "x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(x, x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dense(dff, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### === 3. Train Model ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining model...\")\n",
    "model.fit(train_ds, validation_data=test_ds, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating original model...\")\n",
    "loss, acc = model.evaluate(test_ds)\n",
    "print(f\"Original model accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir = \"saved_model_imdb\"\n",
    "model.save(saved_model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to TFLite: Baseline (no quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nConverting to TFLite (no quantization)...\")\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"model_fp32.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"Size of unquantized model: {os.path.getsize('model_fp32.tflite') / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to TFLite: Dynamic Range Quantization\n",
    "\n",
    "- Dynamic Range Quantization is a post-training quantization technique that reduces model size and improves inference speed.\n",
    "- Weights (the learned parameters) are converted from 32-bit floating point (FP32) to 8-bit integers (INT8).\n",
    "- Activations (intermediate computations during inference) stay in FP32 but the converter records their range so it can quantize them dynamically at runtime.\n",
    "- This method does not require any training or calibration data; it‚Äôs quick and easy to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nConverting to TFLite (dynamic range quantization)...\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "with open(\"model_quant.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "print(f\"Size of quantized model: {os.path.getsize('model_quant.tflite') / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate TFLite Models\n",
    "\n",
    "- Loads the TensorFlow Lite (TFLite) model\n",
    "- Prepares the model‚Äôs input and output details, including expected data types and shapes.\n",
    "- Take a batch of test data (test_ds), convert it to the correct data type for the model.\n",
    "- For each sample in the batch:\n",
    "    - Optionally resize the input tensor if the model expects a dynamic input shape.\n",
    "    - Feed the sample into the TFLite model and run inference.\n",
    "    - Get the model‚Äôs prediction and compare it to the true label to count correct predictions.\n",
    "- Measure total inference time and calculates average latency per sample.\n",
    "- Compute accuracy (correct predictions ratio) and average latency in milliseconds.\n",
    "- Compare the full-precision (FP32) and a quantized TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3GP-1PJ4mf2"
   },
   "outputs": [],
   "source": [
    "def evaluate_tflite_model(tflite_path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Extract shape and dtype expected by TFLite model\n",
    "    input_index = input_details[0]['index']\n",
    "    expected_dtype = input_details[0]['dtype']\n",
    "    expected_shape = input_details[0]['shape']\n",
    "\n",
    "    # Get a single batch of data\n",
    "    test_sample = next(iter(test_ds))\n",
    "    x_sample, y_sample = test_sample\n",
    "    x_sample = x_sample.numpy().astype(expected_dtype)\n",
    "    y_sample = y_sample.numpy()\n",
    "\n",
    "    correct = 0\n",
    "    total = len(x_sample)\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(total):\n",
    "        input_data = x_sample[i:i+1]\n",
    "\n",
    "        # If dynamic shape is enabled, resize input\n",
    "        if -1 in expected_shape:\n",
    "            interpreter.resize_tensor_input(input_index, input_data.shape)\n",
    "            interpreter.allocate_tensors()\n",
    "\n",
    "        interpreter.set_tensor(input_index, input_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        pred = (output[0][0] > 0.5).astype(int)\n",
    "        correct += int(pred == y_sample[i])\n",
    "    end = time.time()\n",
    "\n",
    "    acc = correct / total\n",
    "    latency = (end - start) / total * 1000  # ms/sample\n",
    "    return acc, latency\n",
    "\n",
    "# === Run evaluation ===\n",
    "print(\"\\nEvaluating TFLite models...\")\n",
    "acc_fp32, latency_fp32 = evaluate_tflite_model(\"model_fp32.tflite\")\n",
    "acc_quant, latency_quant = evaluate_tflite_model(\"model_quant.tflite\")\n",
    "\n",
    "print(f\"\\nFP32 Model - Accuracy: {acc_fp32:.4f}, Latency: {latency_fp32:.2f} ms/sample\")\n",
    "print(f\"Quantized Model - Accuracy: {acc_quant:.4f}, Latency: {latency_quant:.2f} ms/sample\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Lab: Quantizing a Pretrained BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Tokenizer and Smallest BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We'll use \"prajjwal1/bert-tiny\" (very small, TF-native)\n",
    "MODEL_NAME = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_model = TFAutoModel.from_pretrained(MODEL_NAME, from_pt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare IMDB Dataset (tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, label):\n",
    "    tokens = tokenizer(\n",
    "        text.numpy().decode('utf-8'),\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    input_ids = tf.convert_to_tensor(tokens['input_ids'][0], dtype=tf.int32)\n",
    "    attention_mask = tf.convert_to_tensor(tokens['attention_mask'][0], dtype=tf.int32)\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    # Return as tuple, NOT dict\n",
    "    return input_ids, attention_mask, label\n",
    "\n",
    "def encode_map(text, label):\n",
    "    # Specify output types as a tuple\n",
    "    input_ids, attention_mask, label = tf.py_function(\n",
    "        encode,\n",
    "        inp=[text, label],\n",
    "        Tout=(tf.int32, tf.int32, tf.float32)\n",
    "    )\n",
    "    # Set shapes (optional but recommended)\n",
    "    input_ids.set_shape([128])\n",
    "    attention_mask.set_shape([128])\n",
    "    label.set_shape([])\n",
    "\n",
    "    # Rebuild dict for model input\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}, label\n",
    "\n",
    "# Load IMDB dataset splits\n",
    "(train_raw, test_raw), ds_info = tfds.load(\n",
    "    'imdb_reviews',\n",
    "    split=['train', 'test'],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "\n",
    "# Now use encode_map\n",
    "train_ds = train_raw.map(encode_map).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_raw.map(encode_map).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Classifier on Top of Frozen Tiny BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze BERT weights\n",
    "bert_model.trainable = False\n",
    "\n",
    "# Define input layers\n",
    "input_ids = tf.keras.Input(shape=(128,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = tf.keras.Input(shape=(128,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "# Run inputs through BERT\n",
    "outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)[1]  # pooled output\n",
    "\n",
    "# Add dropout:\n",
    "x = tf.keras.layers.Dropout(0.2)(outputs)\n",
    "\n",
    "# Add final classification layer\n",
    "x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Build and compile Keras model:\n",
    "model = tf.keras.Model(inputs={\"input_ids\": input_ids, \"attention_mask\": attention_mask}, outputs=x)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Classifier Head Briefly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds, validation_data=test_ds, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save as SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"bert_tiny_classifier\"\n",
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Convert to TFLite with Dynamic Range Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"bert_tiny_quant.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"Quantized model size: {os.path.getsize('bert_tiny_quant.tflite') / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tflite_model(tflite_path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    input_idx = {d['name']: d['index'] for d in input_details}\n",
    "    output_idx = output_details[0]['index']\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for batch in test_ds.take(10):\n",
    "        x_batch, y_batch = batch\n",
    "        for i in range(len(y_batch)):\n",
    "            input_data = {\n",
    "                input_idx['serving_default_input_ids:0']: x_batch['input_ids'][i:i+1].numpy(),\n",
    "                input_idx['serving_default_attention_mask:0']: x_batch['attention_mask'][i:i+1].numpy()\n",
    "            }\n",
    "            for key, val in input_data.items():\n",
    "                interpreter.set_tensor(key, val)\n",
    "            interpreter.invoke()\n",
    "            output = interpreter.get_tensor(output_idx)\n",
    "            pred = (output[0][0] > 0.5).astype(int)\n",
    "            correct += int(pred == y_batch[i].numpy())\n",
    "            total += 1\n",
    "\n",
    "    end = time.time()\n",
    "    acc = correct / total\n",
    "    latency = (end - start) / total * 1000  # ms/sample\n",
    "    print(f\"\\nQuantized TinyBERT Accuracy: {acc:.4f}, Latency: {latency:.2f} ms/sample\")\n",
    "\n",
    "evaluate_tflite_model(\"bert_tiny_quant.tflite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
