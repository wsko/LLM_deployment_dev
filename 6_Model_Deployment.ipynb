{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOAW_nbtGTZY"
   },
   "source": [
    "# LLM Deployment with TensorFlow Serving and Flask API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZ_fdy0jKgHk"
   },
   "source": [
    "## 1. Introduction to Model Serving\n",
    "\n",
    "Model serving refers to making a trained machine learning model available for inference via an interface (usually HTTP or gRPC).\n",
    "\n",
    "### ðŸ”¹ Why Serve Models?\n",
    "- Enable real-time predictions\n",
    "- Allow external applications to use the model\n",
    "- Deploy on scalable infrastructure\n",
    "- Automate inference as part of a system pipeline\n",
    "\n",
    "### ðŸ”¹ Types of Serving\n",
    "- **Batch serving**: Process data in bulk (e.g., nightly jobs)\n",
    "- **Online (real-time) serving**: Respond to incoming prediction requests instantly\n",
    "\n",
    "---\n",
    "\n",
    "## 2. TensorFlow Serving Overview\n",
    "\n",
    "TensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments.\n",
    "\n",
    "### ðŸ”¹ Key Features\n",
    "- Designed for TensorFlow models (also supports others via custom servables)\n",
    "- Supports model versioning\n",
    "- Exposes REST and gRPC APIs\n",
    "\n",
    "### ðŸ”¹ Core Concepts\n",
    "- **SavedModel**: TensorFlowâ€™s standard format for serializing models\n",
    "- **Model versioning**: You can serve multiple versions of a model\n",
    "- **Endpoints**:\n",
    "  - REST: `http://host:port/v1/models/model_name:predict`\n",
    "  - gRPC: `localhost:8500`\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Serving with Flask APIs\n",
    "\n",
    "Flask is a lightweight Python web framework ideal for custom APIs.\n",
    "\n",
    "### ðŸ”¹ Why Use Flask Instead of TF Serving?\n",
    "- Custom preprocessing/postprocessing\n",
    "- Easier debugging and flexibility\n",
    "- Full control over request/response format\n",
    "\n",
    "### ðŸ”¹ Basic Flask Serving Structure\n",
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json()\n",
    "    # Preprocess input\n",
    "    # Run model prediction\n",
    "    # Postprocess output\n",
    "    return jsonify({'prediction': 'result'})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Introduction to Docker for Model Packaging\n",
    "\n",
    "Docker is a tool designed to make it easier to create, deploy, and run applications by using containers.\n",
    "\n",
    "### ðŸ”¹ Why Use Docker?\n",
    "- Portability: Run the same container anywhere\n",
    "- Isolation: Avoid dependency conflicts\n",
    "- Scalability: Easier deployment in cloud or clusters\n",
    "\n",
    "### ðŸ”¹ Core Docker Concepts\n",
    "- **Image**: A snapshot of your application and environment\n",
    "- **Container**: A running instance of an image\n",
    "- **Dockerfile**: Script to build a Docker image\n",
    "\n",
    "### ðŸ”¹ Docker CLI Basics\n",
    "- `docker build -t my-image .`\n",
    "- `docker run -p 8501:8501 my-image`\n",
    "- `docker exec -it <container_id> bash`\n",
    "- `docker logs <container_id>`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWJajmoCGoh-"
   },
   "source": [
    "# Lab: Create Docker Containers for LLM Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73h-TVJXI0aE"
   },
   "source": [
    "---\n",
    "\n",
    "## **Objective**\n",
    "\n",
    "* Package a TensorFlow SavedModel (e.g., `sentiment_model`) into Docker containers.\n",
    "* Understand using the official TensorFlow Serving image with bind mounts.\n",
    "* Build a Flask API container that loads and serves the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Task 1: Create Save and Test a Model**\n",
    "\n",
    "#### Import `distilbert-base-uncased-finetuned-sst-2-english` and save as a TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x0000010D68000650>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x0000010D69E8EF50>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x0000010D69287A50>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x0000010D69E8FF90>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x0000010D69F85450>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x0000010D69EE6610>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: qa_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: qa_model\\assets\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Save model in SavedModel format\n",
    "model.save(\"sentiment_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8t_e0PybILbs"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Task 2: Container for Flask API Serving the Model**\n",
    "\n",
    "### Project Structure\n",
    "\n",
    "```\n",
    "llm-flask/\n",
    "â”œâ”€â”€ app.py                # Flask application code\n",
    "â”œâ”€â”€ sentiment_model/         # TensorFlow SavedModel directory (can be bind-mounted)\n",
    "â”œâ”€â”€ requirements.txt      # Python dependencies\n",
    "â””â”€â”€ Dockerfile            # Container build instructions\n",
    "```\n",
    "\n",
    "### Sample `app.py`\n",
    "\n",
    "```python\n",
    "# app.py\n",
    "from flask import Flask, request, jsonify\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Define the path where the model is saved inside the Docker container\n",
    "MODEL_PATH = \"./sentiment_model\"\n",
    "\n",
    "# Load the tokenizer and model globally to avoid reloading on each request\n",
    "# This assumes the model and tokenizer were saved together or the tokenizer\n",
    "# can be loaded from the same pre-trained name.\n",
    "# Since the user saved the model directory, we'll try to load tokenizer from there too.\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "    print(\"Model and tokenizer loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or tokenizer: {e}\")\n",
    "    # Fallback to original model name if loading from path fails,\n",
    "    # though this might not work if the saved model is truly custom.\n",
    "    # For this specific case (distilbert-base-uncased-finetuned-sst-2-english),\n",
    "    # it's better to load from the saved path.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    print(\"Loaded model and tokenizer from original Hugging Face Hub as a fallback.\")\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    \"\"\"\n",
    "    Home route to confirm the API is running.\n",
    "    \"\"\"\n",
    "    return \"Sentiment Analysis API is running! Use /predict endpoint.\"\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict_sentiment():\n",
    "    \"\"\"\n",
    "    Predicts the sentiment of the provided text.\n",
    "    Expects a JSON payload with a 'text' field.\n",
    "    Example: {\"text\": \"This movie was fantastic!\"}\n",
    "    \"\"\"\n",
    "    if not request.is_json:\n",
    "        return jsonify({\"error\": \"Request must be JSON\"}), 400\n",
    "\n",
    "    data = request.get_json()\n",
    "    text = data.get('text')\n",
    "\n",
    "    if not text:\n",
    "        return jsonify({\"error\": \"No 'text' field found in the request\"}), 400\n",
    "\n",
    "    try:\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\n",
    "\n",
    "        # Perform inference\n",
    "        outputs = model(inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Get probabilities (softmax for classification)\n",
    "        probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "        predicted_class_id = tf.argmax(probabilities, axis=1).numpy()[0]\n",
    "\n",
    "        # The SST-2 dataset has two labels: 0 for negative, 1 for positive\n",
    "        sentiment_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "        predicted_sentiment = sentiment_labels.get(predicted_class_id, \"Unknown\")\n",
    "        confidence = probabilities[0][predicted_class_id].numpy().item() # Convert to standard Python float\n",
    "\n",
    "        return jsonify({\n",
    "            \"text\": text,\n",
    "            \"sentiment\": predicted_sentiment,\n",
    "            \"confidence\": f\"{confidence:.4f}\"\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Prediction error: {e}\")\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Use 0.0.0.0 to make the Flask app accessible from outside the container\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### `requirements.txt`\n",
    "\n",
    "```\n",
    "Flask==2.3.2\n",
    "transformers==4.30.2\n",
    "tensorflow==2.13.0\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Dockerfile for Flask API\n",
    "\n",
    "```dockerfile\n",
    "# Dockerfile\n",
    "\n",
    "# Use a lightweight Python base image\n",
    "FROM python:3.9-slim-buster\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the requirements file and install dependencies\n",
    "# We install tensorflow-cpu to reduce image size unless GPU is specifically needed\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt \\\n",
    "    && pip install tensorflow-cpu==2.13.0 # Explicitly install CPU version of TensorFlow\n",
    "\n",
    "# Copy the Flask application and the saved model directory\n",
    "COPY app.py .\n",
    "COPY sentiment_model ./sentiment_model\n",
    "\n",
    "# Expose the port that the Flask app will run on\n",
    "EXPOSE 5000\n",
    "\n",
    "# Define the command to run the Flask application\n",
    "# Using gunicorn for a production-ready WSGI server\n",
    "# Install gunicorn first\n",
    "RUN pip install gunicorn\n",
    "\n",
    "# Command to run the Flask application with gunicorn\n",
    "# -b 0.0.0.0:5000 binds to all network interfaces on port 5000\n",
    "# app:app refers to the 'app' variable in 'app.py'\n",
    "CMD [\"gunicorn\", \"-b\", \"0.0.0.0:5000\", \"app:app\"]\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "- Organize your files:\n",
    "\n",
    "```\n",
    "sentiment_app/\n",
    "â”œâ”€â”€ app.py\n",
    "â”œâ”€â”€ Dockerfile\n",
    "â”œâ”€â”€ requirements.txt\n",
    "â””â”€â”€ sentiment_model/\n",
    "    â”œâ”€â”€ assets/\n",
    "    â”œâ”€â”€ keras_metadata.pb\n",
    "    â”œâ”€â”€ saved_model.pb\n",
    "    â””â”€â”€ variables/\n",
    "        â””â”€â”€ variables.data-00000-of-00001\n",
    "        â””â”€â”€ variables.index\n",
    "```\n",
    "\n",
    "\n",
    "### Build and Run Flask Container\n",
    "\n",
    "1. Build:\n",
    "\n",
    "```bash\n",
    "docker build -t sentiment-api .\n",
    "```\n",
    "\n",
    "2. Run (bind mount model directory if you prefer not to bake it into the image):\n",
    "\n",
    "```bash\n",
    "docker run -p 5000:5000 sentiment-api\n",
    "```\n",
    "\n",
    "3. Test your deployment:\n",
    "\n",
    "```bash\n",
    "curl -X POST -H \"Content-Type: application/json\" -d \"{\\\"text\\\": \\\"I absolutely loved this movie, it was fantastic!\\\"}\" http://localhost:5000/predict\n",
    "```\n",
    "\n",
    "4. Expected Output:\n",
    "```json\n",
    "{\n",
    "  \"confidence\": \"0.9998\",\n",
    "  \"sentiment\": \"Positive\",\n",
    "  \"text\": \"I absolutely loved this movie, it was fantastic!\"\n",
    "}\n",
    "```\n",
    "---\n",
    "* For the TensorFlow Serving container, the model directory must contain a versioned subfolder (e.g., `/models/pruned_model/1/`) with the SavedModel files.\n",
    "* For Flask serving, the model can be loaded from a local folder; ensure your `app.py` path matches where the model is mounted or copied.\n",
    "* Test your REST API endpoints with curl or Postman:\n",
    "\n",
    "  * TensorFlow Serving example prediction URL: `http://localhost:8501/v1/models/pruned_model:predict`\n",
    "  * Flask API example prediction URL: `http://localhost:5000/predict`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf8HdoVaI5ar"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Task 3: Container for TensorFlow Serving with SavedModel**\n",
    "\n",
    "\n",
    "### Prepare Your Model for TensorFlow Serving\n",
    "\n",
    "- TensorFlow Serving expects models to be organized in a specific directory structure: model_name/version_number/.\n",
    "- Create a new directory for your TensorFlow Serving setup, let's call it tfserving-sentiment.\n",
    "- Inside tfserving-sentiment, create another directory named sentiment_model (this is the model_name).\n",
    "- Inside sentiment_model, create a version directory, typically 1 (or any integer representing the version).\n",
    "- Move your saved sentiment_model content into this version directory.\n",
    "\n",
    "```\n",
    "tfserving-sentiment/   <-- NEW DIRECTORY\n",
    "â””â”€â”€ sentiment_model/   <-- Model name for TF Serving\n",
    "    â””â”€â”€ 1/             <-- Model version\n",
    "        â””â”€â”€ assets/\n",
    "        â””â”€â”€ keras_metadata.pb\n",
    "        â””â”€â”€ saved_model.pb\n",
    "        â””â”€â”€ variables/\n",
    "            â””â”€â”€ variables.data-00000-of-00001\n",
    "            â””â”€â”€ variables.index\n",
    "```\n",
    "\n",
    "\n",
    "### Create the Dockerfile.tfserving\n",
    "We'll use an official TensorFlow Serving Docker image:\n",
    "\n",
    "\n",
    "```dockerfile\n",
    "# Dockerfile.tfserving (Simplified for Bind Mount)\n",
    "\n",
    "# Use the official TensorFlow Serving base image\n",
    "# This image comes with TensorFlow Serving installed and configured.\n",
    "FROM tensorflow/serving\n",
    "\n",
    "# Expose the default gRPC (8500) and REST (8501) ports\n",
    "EXPOSE 8500\n",
    "EXPOSE 8501\n",
    "\n",
    "# Command to run TensorFlow Serving.\n",
    "# Note: --model_base_path will point to the location *inside* the container\n",
    "# where we will bind mount our model directory.\n",
    "CMD [\"/usr/bin/tensorflow_model_server\", \\\n",
    "     \"--rest_api_port=8501\", \\\n",
    "     \"--model_name=sentiment_model\", \\\n",
    "     \"--model_base_path=/models/sentiment_model\"]\n",
    "\n",
    "```\n",
    "\n",
    "- **Place this `Dockerfile.tfserving` file inside your tfserving-sentiment directory.**\n",
    "\n",
    "\n",
    "\n",
    "### Create a Python Script for Prediction (REST API)\n",
    "\n",
    "- This script will show you how to send a request to the TensorFlow Serving REST API:\n",
    "\n",
    "```python\n",
    "# predict_tfserving.py\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# TensorFlow Serving REST API endpoint\n",
    "# Replace localhost with your server IP if running remotely\n",
    "TF_SERVING_REST_URL = \"http://localhost:8501/v1/models/sentiment_model:predict\"\n",
    "\n",
    "def predict_sentiment_tfserving(text_input):\n",
    "    \"\"\"\n",
    "    Sends a text input to the TensorFlow Serving REST API for sentiment prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        from transformers import AutoTokenizer\n",
    "        import tensorflow as tf # Required for tf.constant conversion\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "        inputs = tokenizer(text_input, return_tensors=\"tf\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "        # Convert TensorFlow tensors to Python lists for JSON serialization\n",
    "        input_ids_list = inputs[\"input_ids\"].numpy().tolist()\n",
    "        attention_mask_list = inputs[\"attention_mask\"].numpy().tolist()\n",
    "\n",
    "        # Construct the payload for TensorFlow Serving\n",
    "        # Each element in 'instances' is a single prediction request.\n",
    "        # It should be a dictionary where keys match the model's input names.\n",
    "        payload = {\n",
    "            \"instances\": [\n",
    "                {\n",
    "                    \"input_ids\": input_ids_list[0],  # Get the first (and only) batch item\n",
    "                    \"attention_mask\": attention_mask_list[0]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Error: 'transformers' and 'tensorflow' libraries are required in the client environment for tokenization.\")\n",
    "        print(\"Please install them: pip install transformers tensorflow\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during tokenization: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(TF_SERVING_REST_URL, headers=headers, data=json.dumps(payload))\n",
    "        response.raise_for_status() # Raise an exception for HTTP errors (4xx or 5xx)\n",
    "        prediction_result = response.json()\n",
    "\n",
    "        # The output from TF Serving will be the raw logits.\n",
    "        # We need to apply softmax and get the predicted class.\n",
    "        # The output structure is typically {\"predictions\": [[logit_0, logit_1]]}\n",
    "        logits = np.array(prediction_result[\"predictions\"][0])\n",
    "        probabilities = np.exp(logits) / np.sum(np.exp(logits)) # Manual softmax\n",
    "        predicted_class_id = np.argmax(probabilities)\n",
    "\n",
    "        sentiment_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "        predicted_sentiment = sentiment_labels.get(predicted_class_id, \"Unknown\")\n",
    "        confidence = probabilities[predicted_class_id]\n",
    "\n",
    "        return predicted_sentiment, confidence\n",
    "\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"Error: Could not connect to TensorFlow Serving at {TF_SERVING_REST_URL}.\")\n",
    "        print(\"Please ensure the Docker container is running and accessible.\")\n",
    "        return None, None\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err} - {response.text}\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_text_positive = \"This movie was absolutely brilliant and I loved every minute of it!\"\n",
    "    test_text_negative = \"I hated this product, it was a complete waste of money.\"\n",
    "\n",
    "    print(f\"Testing positive sentiment: '{test_text_positive}'\")\n",
    "    sentiment, confidence = predict_sentiment_tfserving(test_text_positive)\n",
    "    if sentiment:\n",
    "        print(f\"Sentiment: {sentiment}, Confidence: {confidence:.4f}\\n\")\n",
    "\n",
    "    print(f\"Testing negative sentiment: '{test_text_negative}'\")\n",
    "    sentiment, confidence = predict_sentiment_tfserving(test_text_negative)\n",
    "    if sentiment:\n",
    "        print(f\"Sentiment: {sentiment}, Confidence: {confidence:.4f}\\n\")\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "- **Place this `predict_tfserving.py` file inside your tfserving-sentiment directory.**\n",
    "\n",
    "### Step-by-Step Deployment and Testing\n",
    "\n",
    "- **Navigate to project Dir:**\n",
    "\n",
    "```bash\n",
    "cd tfserving-sentiment\n",
    "```\n",
    "\n",
    "- **Build the TensorFlow Serving Docker Image:**\n",
    "\n",
    "```bash\n",
    "docker build -f Dockerfile.tfserving -t sentiment-tfserving .\n",
    "```\n",
    "`-f Dockerfile.tfserving`: Specifies that we are using Dockerfile.tfserving instead of the default Dockerfile.\n",
    "\n",
    "`-t sentiment-tfserving`: Tags the image with the name sentiment-tfserving.\n",
    "\n",
    "`.`: Specifies the build context (the current directory), which is where your sentiment_model directory (containing the 1 version folder) should be.\n",
    "\n",
    "\n",
    "\n",
    "- **Run TF Serving container:**\n",
    "\n",
    "```bash\n",
    "docker run -p 8501:8501 -p 8500:8500 --name tf-sentiment-server sentiment-tfserving\n",
    "```\n",
    "\n",
    "`-p 8501:8501`: Maps the container's REST API port to your host's port 8501.\n",
    "\n",
    "`-p 8500:8500`: Maps the container's gRPC port to your host's port 8500 (optional for this example, but good practice).\n",
    "\n",
    "`--name tf-sentiment-server`: Gives your running container a friendly name.\n",
    "\n",
    "`sentiment-tfserving`: The name of the Docker image you just built.\n",
    "\n",
    "> You should see TensorFlow Serving startup logs in your terminal, indicating that it's loading the sentiment_model at version 1.\n",
    "\n",
    "### Run prediction script\n",
    "\n",
    "```bash\n",
    "python predict_tfserving.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Deploying LLMs with TensorFlow Serving and with Flask API on AWS ECS\n",
    "-----\n",
    "\n",
    "### Pre-deployment Checklist \n",
    "\n",
    "Before you touch the AWS Console, you need to prepare your Docker images and your model locally:\n",
    "\n",
    "1.  **Build Docker Images Locally:**\n",
    "\n",
    "      * Ensure you have your `sentiment-api` Docker image (for Flask).\n",
    "      * Ensure you have your `sentiment-tfserving` Docker image (for TensorFlow Serving).\n",
    "          * Remember, for `sentiment-tfserving`, your `Dockerfile.tfserving` should be the **simplified version** (without `COPY sentiment_model`) because we'll be loading the model from S3.\n",
    "\n",
    "2.  **Prepare Your Model for S3:**\n",
    "\n",
    "      * Make sure your saved sentiment model is organized locally in the `sentiment_model/1/` structure (e.g., `tfserving-sentiment/sentiment_model/1/`).\n",
    "\n",
    "-----\n",
    "\n",
    "### Step-by-Step AWS Management Console Deployment\n",
    "\n",
    "-----\n",
    "\n",
    "### Part 1: Common AWS Setup (ECR, IAM, Networking)\n",
    "\n",
    "These steps are done once for both deployments.\n",
    "\n",
    "#### 1\\. Push Docker Images to Amazon ECR\n",
    "\n",
    "1.  **Go to ECR:**\n",
    "\n",
    "      * Open your web browser and navigate to the [AWS Management Console](https://aws.amazon.com/console/).\n",
    "      * Search for \"ECR\" in the search bar and click on \"Elastic Container Registry\".\n",
    "\n",
    "2.  **Create Repositories:**\n",
    "\n",
    "      * Click the **\"Create repository\"** button.\n",
    "      * **For Flask App:**\n",
    "          * **Visibility settings:** Keep \"Private\".\n",
    "          * **Repository name:** Enter `sentiment-flask-app`.\n",
    "          * Leave other options as default.\n",
    "          * Click **\"Create repository\"**.\n",
    "      * **For TF Serving:**\n",
    "          * Click **\"Create repository\"** again.\n",
    "          * **Repository name:** Enter `sentiment-tfserving-model`.\n",
    "          * Click **\"Create repository\"**.\n",
    "\n",
    "3.  **Get Push Commands (and run them locally):**\n",
    "\n",
    "      * On the ECR Repositories page, click on `sentiment-flask-app`.\n",
    "      * Click the **\"View push commands\"** button.\n",
    "      * **Copy the commands one by one and run them in your local terminal.** This will authenticate Docker and push your `sentiment-api:latest` image to ECR.\n",
    "      * Repeat this process for the `sentiment-tfserving-model` repository, pushing your `sentiment-tfserving:latest` image.\n",
    "      * **Wait for both pushes to complete.**\n",
    "\n",
    "#### 2\\. Create IAM Role for ECS Task Execution\n",
    "\n",
    "This role allows Fargate tasks to pull images and send logs.\n",
    "\n",
    "1.  **Go to IAM:**\n",
    "\n",
    "      * Search for \"IAM\" in the console search bar and click on \"IAM\".\n",
    "      * In the left navigation pane, click **\"Roles\"**.\n",
    "\n",
    "2.  **Create Role:**\n",
    "\n",
    "      * Click the **\"Create role\"** button.\n",
    "      * **Trusted entity type:** Select \"AWS service\".\n",
    "      * **Use case:** Select \"Elastic Container Service\" from the list, then choose \"Elastic Container Service Task\".\n",
    "      * Click **\"Next\"**.\n",
    "      * **Permissions policies:** Search for `AmazonECSTaskExecutionRolePolicy`. Check the box next to it.\n",
    "      * Click **\"Next\"**.\n",
    "      * **Role details:**\n",
    "          * **Role name:** Enter `ecsTaskExecutionRole` (if you don't have one already).\n",
    "          * Review and click **\"Create role\"**.\n",
    "\n",
    "#### 3\\. Identify VPC and Public Subnets\n",
    "\n",
    "We'll use your default VPC for simplicity.\n",
    "\n",
    "1.  **Go to VPC:**\n",
    "      * Search for \"VPC\" in the console search bar and click on \"VPC\".\n",
    "      * In the left navigation pane, click **\"Your VPCs\"**. Note down the **VPC ID** of your \"Default\" VPC.\n",
    "      * In the left navigation pane, click **\"Subnets\"**.\n",
    "      * Filter the subnets by your Default VPC ID.\n",
    "      * Identify at least **two subnets** that have \"Yes\" under \"Auto-assign public IPv4 address\" or whose route table has a route to an Internet Gateway. Note down their **Subnet IDs**. These are your public subnets.\n",
    "\n",
    "#### 4\\. Create Security Groups\n",
    "\n",
    "These control network traffic to your ALB and Fargate tasks.\n",
    "\n",
    "1.  **Go to EC2:**\n",
    "\n",
    "      * Search for \"EC2\" in the console search bar and click on \"EC2\".\n",
    "      * In the left navigation pane, scroll down to \"Network & Security\" and click **\"Security Groups\"**.\n",
    "\n",
    "2.  **Create ALB Security Group:**\n",
    "\n",
    "      * Click **\"Create security group\"**.\n",
    "      * **Basic details:**\n",
    "          * **Security group name:** `sentiment-alb-sg-demo`\n",
    "          * **Description:** `Security group for sentiment demo ALB`\n",
    "          * **VPC:** Select your Default VPC ID.\n",
    "      * **Inbound rules:**\n",
    "          * Click **\"Add rule\"**.\n",
    "          * **Type:** \"HTTP\", **Source:** \"Anywhere-IPv4\" (`0.0.0.0/0`).\n",
    "          * Click **\"Add rule\"** again.\n",
    "          * **Type:** \"HTTPS\", **Source:** \"Anywhere-IPv4\" (`0.0.0.0/0`) (Good practice, even if not using HTTPS yet).\n",
    "      * Leave outbound rules as default.\n",
    "      * Click **\"Create security group\"**. Note its **Security Group ID**.\n",
    "\n",
    "3.  **Create Task Security Group:**\n",
    "\n",
    "      * Click **\"Create security group\"** again.\n",
    "      * **Basic details:**\n",
    "          * **Security group name:** `sentiment-task-sg-demo`\n",
    "          * **Description:** `Security group for sentiment demo Fargate tasks`\n",
    "          * **VPC:** Select your Default VPC ID.\n",
    "      * **Inbound rules:**\n",
    "          * Click **\"Add rule\"**.\n",
    "          * **Type:** \"Custom TCP\".\n",
    "          * **Port range:** `5000` (for Flask)\n",
    "          * **Source:** Select \"Custom\" and type the **Security Group ID of `sentiment-alb-sg-demo`** (it should auto-complete as you type). This allows traffic only from the ALB.\n",
    "          * Click **\"Add rule\"** again.\n",
    "          * **Type:** \"Custom TCP\".\n",
    "          * **Port range:** `8501` (for TF Serving)\n",
    "          * **Source:** Select \"Custom\" and type the **Security Group ID of `sentiment-alb-sg-demo`**.\n",
    "      * Leave outbound rules as default.\n",
    "      * Click **\"Create security group\"**. Note its **Security Group ID**.\n",
    "\n",
    "#### 5\\. Create ECS Cluster\n",
    "\n",
    "1.  **Go to ECS:**\n",
    "\n",
    "      * Search for \"ECS\" in the console search bar and click on \"Elastic Container Service\".\n",
    "      * In the left navigation pane, click **\"Clusters\"**.\n",
    "\n",
    "2.  **Create Cluster:**\n",
    "\n",
    "      * Click **\"Create cluster\"**.\n",
    "      * **Cluster name:** Enter `sentiment-cluster-demo`.\n",
    "      * **Infrastructure:** Select \"AWS Fargate (serverless)\".\n",
    "      * Click **\"Create\"**.\n",
    "\n",
    "#### 6\\. Create Application Load Balancer (ALB) and Target Groups\n",
    "\n",
    "We'll create one ALB and two target groups (one for Flask, one for TF Serving).\n",
    "\n",
    "1.  **Go to EC2:**\n",
    "\n",
    "      * Search for \"EC2\" and click on \"EC2\".\n",
    "      * In the left navigation pane, scroll down to \"Load Balancing\" and click **\"Load Balancers\"**.\n",
    "\n",
    "2.  **Create ALB:**\n",
    "\n",
    "      * Click **\"Create Load Balancer\"**.\n",
    "      * Select **\"Application Load Balancer\"** and click \"Create\".\n",
    "      * **Basic configuration:**\n",
    "          * **Load balancer name:** `sentiment-demo-alb`.\n",
    "          * **Scheme:** \"Internet-facing\".\n",
    "          * **IP address type:** \"IPv4\".\n",
    "      * **Network mapping:**\n",
    "          * **VPC:** Select your Default VPC.\n",
    "          * **Mappings:** Select the two public Subnet IDs you identified earlier.\n",
    "      * **Security groups:** Select your `sentiment-alb-sg-demo`.\n",
    "      * **Listeners and routing:**\n",
    "          * **Protocol:** \"HTTP\", **Port:** `80`.\n",
    "          * **Default action:** We will update this later. For now, select \"Create new target group\".\n",
    "              * **Target group name:** `sentiment-flask-tg-demo`.\n",
    "              * **Protocol:** \"HTTP\", **Port:** `5000`.\n",
    "              * **VPC:** Select your Default VPC.\n",
    "              * **Health checks:** Keep default (HTTP, path `/`).\n",
    "              * Click **\"Create target group\"**.\n",
    "          * Go back to the ALB creation tab. For the HTTP:80 listener, under \"Default action\", select **\"Forward to target groups\"** and choose `sentiment-flask-tg-demo`.\n",
    "      * Click **\"Create load balancer\"**.\n",
    "      * **Wait for the ALB to provision (it will say \"active\"). Note down its DNS name.**\n",
    "\n",
    "-----\n",
    "\n",
    "### Part 2: Deploy Flask App on Fargate\n",
    "\n",
    "#### 1\\. Create CloudWatch Log Group for Flask\n",
    "\n",
    "1.  **Go to CloudWatch:**\n",
    "      * Search for \"CloudWatch\" and click on \"CloudWatch\".\n",
    "      * In the left navigation pane, click **\"Log groups\"** (under \"Logs\").\n",
    "      * Click **\"Create log group\"**.\n",
    "      * **Log group name:** `/ecs/sentiment-flask-task-demo`.\n",
    "      * Click **\"Create\"**.\n",
    "\n",
    "#### 2\\. Create ECS Task Definition for Flask\n",
    "\n",
    "1.  **Go to ECS:**\n",
    "\n",
    "      * Search for \"ECS\" and click on \"Elastic Container Service\".\n",
    "      * In the left navigation pane, click **\"Task Definitions\"**.\n",
    "\n",
    "2.  **Create New Task Definition:**\n",
    "\n",
    "      * Click **\"Create new task definition\"**.\n",
    "      * **Launch type compatibility:** Select **\"Fargate\"**.\n",
    "      * Click **\"Next step\"**.\n",
    "      * **Task Definition name:** `sentiment-flask-task-demo`.\n",
    "      * **Task role:** (Leave as `None` for this demo).\n",
    "      * **Task execution role:** Select `ecsTaskExecutionRole`.\n",
    "      * **Task size:**\n",
    "          * **Task CPU (vCPU):** `0.5 vCPU (512)`\n",
    "          * **Task memory (GB):** `1 GB (1024)`\n",
    "      * **Container Definitions:** Click **\"Add container\"**.\n",
    "          * **Container name:** `sentiment-flask-container`.\n",
    "          * **Image:** Paste the ECR URI for your `sentiment-flask-app` image (e.g., `your-aws-account-id.dkr.ecr.your-aws-region.amazonaws.com/sentiment-flask-app:latest`).\n",
    "          * **Port mappings:**\n",
    "              * **Host port:** `5000` (leave blank, Fargate handles this)\n",
    "              * **Container port:** `5000`\n",
    "              * **Protocol:** `tcp`\n",
    "          * **Essential:** Keep checked.\n",
    "          * **Log configuration:**\n",
    "              * **Log driver:** Select `awslogs`.\n",
    "              * **Options:**\n",
    "                  * `awslogs-group`: `/ecs/sentiment-flask-task-demo`\n",
    "                  * `awslogs-region`: Your AWS region (e.g., `us-east-1`)\n",
    "                  * `awslogs-stream-prefix`: `ecs`\n",
    "      * Click **\"Add\"**.\n",
    "      * Review and click **\"Create\"**.\n",
    "\n",
    "#### 3\\. Create ECS Service for Flask\n",
    "\n",
    "1.  **Go to ECS:**\n",
    "\n",
    "      * In the left navigation pane, click **\"Clusters\"**.\n",
    "      * Click on your `sentiment-cluster-demo`.\n",
    "      * Go to the **\"Services\"** tab.\n",
    "\n",
    "2.  **Create Service:**\n",
    "\n",
    "      * Click **\"Create\"**.\n",
    "      * **Configure service:**\n",
    "          * **Launch type:** \"Fargate\".\n",
    "          * **Task Definition:** Select `sentiment-flask-task-demo` (and its latest revision).\n",
    "          * **Service name:** `sentiment-flask-service-demo`.\n",
    "          * **Desired tasks:** `1`.\n",
    "      * **Networking:**\n",
    "          * **VPC:** Select your Default VPC.\n",
    "          * **Subnets:** Select the two public Subnet IDs you identified.\n",
    "          * **Security groups:** Select your `sentiment-task-sg-demo`.\n",
    "          * **Auto-assign public IP:** Select **\"ENABLED\"**.\n",
    "      * **Load balancing:**\n",
    "          * **Load balancer type:** Select \"Application Load Balancer\".\n",
    "          * **Load balancer name:** Select `sentiment-demo-alb`.\n",
    "          * **Container to load balance:** Click \"Add container to load balancer\" and select `sentiment-flask-container:5000`.\n",
    "          * **Target group name:** Select `sentiment-flask-tg-demo`.\n",
    "      * Review and click **\"Create Service\"**.\n",
    "      * **Wait for the service to become \"ACTIVE\"** (this can take a few minutes as tasks provision).\n",
    "\n",
    "#### 4\\. Test Flask App Deployment\n",
    "\n",
    "1.  **Get ALB DNS Name:**\n",
    "      * Go to EC2 -\\> Load Balancers.\n",
    "      * Select `sentiment-demo-alb`.\n",
    "      * Copy its **DNS name**.\n",
    "2.  **Test with `curl` (from your local terminal):**\n",
    "    ```bash\n",
    "    curl -X POST -H \"Content-Type: application/json\" \\\n",
    "         -d \"{\\\"text\\\": \\\"This movie was absolutely brilliant!\\\"}\" \\\n",
    "         http://<ALB-DNS-NAME>/predict\n",
    "    ```\n",
    "    (Replace `<ALB-DNS-NAME>` with the actual DNS name). You should get a JSON response.\n",
    "\n",
    "-----\n",
    "\n",
    "### Part 3: Deploy TensorFlow Serving on Fargate\n",
    "\n",
    "#### 1\\. Upload Model to S3\n",
    "\n",
    "1.  **Go to S3:**\n",
    "\n",
    "      * Search for \"S3\" and click on \"S3\".\n",
    "\n",
    "2.  **Create Bucket:**\n",
    "\n",
    "      * Click **\"Create bucket\"**.\n",
    "      * **Bucket name:** Enter a **globally unique** name (e.g., `your-aws-account-id-sentiment-model-demo`).\n",
    "      * **AWS Region:** Select your AWS region.\n",
    "      * Leave other options as default.\n",
    "      * Click **\"Create bucket\"**.\n",
    "\n",
    "3.  **Upload Model Files:**\n",
    "\n",
    "      * Click on your newly created bucket.\n",
    "      * Click **\"Upload\"**.\n",
    "      * Click **\"Add folder\"**.\n",
    "      * Select your local `tfserving-sentiment/sentiment_model/` folder (the one containing the `1` version folder).\n",
    "      * Click **\"Upload\"**. Confirm the upload. Ensure the structure `sentiment_model/1/` is preserved in S3.\n",
    "\n",
    "#### 2\\. Update IAM Role for S3 Access\n",
    "\n",
    "Your `ecsTaskExecutionRole` needs permission to read from S3.\n",
    "\n",
    "1.  **Go to IAM:**\n",
    "\n",
    "      * Search for \"IAM\" and click on \"IAM\".\n",
    "      * In the left navigation pane, click **\"Roles\"**.\n",
    "      * Find and click on your `ecsTaskExecutionRole`.\n",
    "\n",
    "2.  **Attach S3 Policy:**\n",
    "\n",
    "      * On the \"Permissions\" tab, click **\"Add permissions\"** -\\> **\"Attach policies\"**.\n",
    "      * Search for `AmazonS3ReadOnlyAccess`.\n",
    "      * Check the box next to it.\n",
    "      * Click **\"Add permissions\"**.\n",
    "\n",
    "#### 3\\. Create CloudWatch Log Group for TF Serving\n",
    "\n",
    "1.  **Go to CloudWatch:**\n",
    "      * Search for \"CloudWatch\" and click on \"CloudWatch\".\n",
    "      * In the left navigation pane, click **\"Log groups\"** (under \"Logs\").\n",
    "      * Click **\"Create log group\"**.\n",
    "      * **Log group name:** `/ecs/sentiment-tfserving-task-demo`.\n",
    "      * Click **\"Create\"**.\n",
    "\n",
    "#### 4\\. Add New Listener Rule to Existing ALB\n",
    "\n",
    "This routes requests with a `/tfserve` path to your TF Serving container.\n",
    "\n",
    "1.  **Go to EC2:**\n",
    "\n",
    "      * Search for \"EC2\" and click on \"EC2\".\n",
    "      * In the left navigation pane, scroll down to \"Load Balancing\" and click **\"Load Balancers\"**.\n",
    "      * Select your `sentiment-demo-alb`.\n",
    "      * Go to the **\"Listeners\"** tab.\n",
    "      * Select the \"HTTP : 80\" listener and click **\"View/edit rules\"**.\n",
    "      * Click the **\"+\"** icon (to insert a rule) or **\"Insert Rule\"**.\n",
    "\n",
    "2.  **Configure Rule:**\n",
    "\n",
    "      * Click **\"Add condition\"** -\\> **\"Path\"**.\n",
    "      * **Path pattern:** Enter `/tfserve*`.\n",
    "      * Click **\"Add action\"** -\\> **\"Forward to\"**.\n",
    "      * Select **\"Create new target group\"**.\n",
    "          * **Target group name:** `sentiment-tfserving-tg-demo`.\n",
    "          * **Protocol:** \"HTTP\", **Port:** `8501`.\n",
    "          * **VPC:** Select your Default VPC.\n",
    "          * **Health checks:** Path `/v1/models/sentiment_model`.\n",
    "          * Click **\"Create target group\"**.\n",
    "      * Go back to the rule configuration. For the \"Forward to\" action, select your newly created `sentiment-tfserving-tg-demo`.\n",
    "      * Click the **\"Save\"** button (top right).\n",
    "\n",
    "#### 5\\. Create ECS Task Definition for TF Serving\n",
    "\n",
    "1.  **Go to ECS:**\n",
    "\n",
    "      * Search for \"ECS\" and click on \"Elastic Container Service\".\n",
    "      * In the left navigation pane, click **\"Task Definitions\"**.\n",
    "\n",
    "2.  **Create New Task Definition:**\n",
    "\n",
    "      * Click **\"Create new task definition\"**.\n",
    "      * **Launch type compatibility:** Select **\"Fargate\"**.\n",
    "      * Click **\"Next step\"**.\n",
    "      * **Task Definition name:** `sentiment-tfserving-task-demo`.\n",
    "      * **Task execution role:** Select `ecsTaskExecutionRole` (which now has S3 read access).\n",
    "      * **Task size:**\n",
    "          * **Task CPU (vCPU):** `0.5 vCPU (512)`\n",
    "          * **Task memory (GB):** `1 GB (1024)`\n",
    "      * **Container Definitions:** Click **\"Add container\"**.\n",
    "          * **Container name:** `sentiment-tfserving-container`.\n",
    "          * **Image:** Paste the ECR URI for your `sentiment-tfserving-model` image.\n",
    "          * **Port mappings:**\n",
    "              * **Host port:** `8501` (leave blank)\n",
    "              * **Container port:** `8501`\n",
    "              * **Protocol:** `tcp`\n",
    "          * **Essential:** Keep checked.\n",
    "          * **Command:** This is critical. Enter the command as a JSON array of strings, separated by commas (no spaces between elements in the array):\n",
    "            ```\n",
    "            [\"/usr/bin/tensorflow_model_server\",\"--rest_api_port=8501\",\"--model_name=sentiment_model\",\"--model_base_path=s3://your-aws-account-id-sentiment-model-demo/sentiment_model\"]\n",
    "            ```\n",
    "            (Replace `your-aws-account-id-sentiment-model-demo` with your actual S3 bucket name).\n",
    "          * **Log configuration:**\n",
    "              * **Log driver:** Select `awslogs`.\n",
    "              * **Options:**\n",
    "                  * `awslogs-group`: `/ecs/sentiment-tfserving-task-demo`\n",
    "                  * `awslogs-region`: Your AWS region\n",
    "                  * `awslogs-stream-prefix`: `ecs`\n",
    "      * Click **\"Add\"**.\n",
    "      * Review and click **\"Create\"**.\n",
    "\n",
    "#### 6\\. Create ECS Service for TF Serving\n",
    "\n",
    "1.  **Go to ECS:**\n",
    "\n",
    "      * In the left navigation pane, click **\"Clusters\"**.\n",
    "      * Click on your `sentiment-cluster-demo`.\n",
    "      * Go to the **\"Services\"** tab.\n",
    "\n",
    "2.  **Create Service:**\n",
    "\n",
    "      * Click **\"Create\"**.\n",
    "      * **Configure service:**\n",
    "          * **Launch type:** \"Fargate\".\n",
    "          * **Task Definition:** Select `sentiment-tfserving-task-demo` (and its latest revision).\n",
    "          * **Service name:** `sentiment-tfserving-service-demo`.\n",
    "          * **Desired tasks:** `1`.\n",
    "      * **Networking:**\n",
    "          * **VPC:** Select your Default VPC.\n",
    "          * **Subnets:** Select the two public Subnet IDs you identified.\n",
    "          * **Security groups:** Select your `sentiment-task-sg-demo`.\n",
    "          * **Auto-assign public IP:** Select **\"ENABLED\"**.\n",
    "      * **Load balancing:**\n",
    "          * **Load balancer type:** Select \"Application Load Balancer\".\n",
    "          * **Load balancer name:** Select `sentiment-demo-alb`.\n",
    "          * **Container to load balance:** Click \"Add container to load balancer\" and select `sentiment-tfserving-container:8501`.\n",
    "          * **Target group name:** Select `sentiment-tfserving-tg-demo`.\n",
    "      * Review and click **\"Create Service\"**.\n",
    "      * **Wait for the service to become \"ACTIVE\"**.\n",
    "\n",
    "#### 7\\. Test TF Serving Deployment\n",
    "\n",
    "1.  **Get ALB DNS Name:**\n",
    "      * Go to EC2 -\\> Load Balancers.\n",
    "      * Select `sentiment-demo-alb`.\n",
    "      * Copy its **DNS name**.\n",
    "2.  **Update `predict_tfserving.py` (locally):**\n",
    "      * Open your `predict_tfserving.py` script.\n",
    "      * Change the `TF_SERVING_REST_URL` to:\n",
    "        ```python\n",
    "        TF_SERVING_REST_URL = \"http://<ALB-DNS-NAME>/tfserve/v1/models/sentiment_model:predict\"\n",
    "        ```\n",
    "        (Replace `<ALB-DNS-NAME>` with the actual DNS name).\n",
    "3.  **Run `predict_tfserving.py` (from your local terminal):**\n",
    "    ```bash\n",
    "    python predict_tfserving.py\n",
    "    ```\n",
    "    You should see the sentiment predictions.\n",
    "\n",
    "-----\n",
    "\n",
    "### Cleanup (Important to Avoid Charges\\!)\n",
    "\n",
    "When you're done with the lab:\n",
    "\n",
    "1.  **Delete ECS Services:**\n",
    "      * Go to ECS -\\> Clusters -\\> `sentiment-cluster-demo`.\n",
    "      * Go to the \"Services\" tab.\n",
    "      * Select `sentiment-flask-service-demo` and click \"Delete\". Confirm.\n",
    "      * Select `sentiment-tfserving-service-demo` and click \"Delete\". Confirm.\n",
    "2.  **Delete ECS Task Definitions:**\n",
    "      * Go to ECS -\\> Task Definitions.\n",
    "      * Select `sentiment-flask-task-demo` and `sentiment-tfserving-task-demo`.\n",
    "      * Click \"Actions\" -\\> \"Deregister task definition\".\n",
    "3.  **Delete Load Balancer & Target Groups:**\n",
    "      * Go to EC2 -\\> Load Balancers.\n",
    "      * Select `sentiment-demo-alb` and click \"Actions\" -\\> \"Delete load balancer\". Confirm.\n",
    "      * Go to EC2 -\\> Target Groups.\n",
    "      * Select `sentiment-flask-tg-demo` and `sentiment-tfserving-tg-demo`. Click \"Actions\" -\\> \"Delete\". Confirm.\n",
    "4.  **Delete ECS Cluster:**\n",
    "      * Go to ECS -\\> Clusters.\n",
    "      * Select `sentiment-cluster-demo` and click \"Delete Cluster\". Confirm.\n",
    "5.  **Delete Security Groups:**\n",
    "      * Go to EC2 -\\> Security Groups.\n",
    "      * Select `sentiment-alb-sg-demo` and `sentiment-task-sg-demo`. Click \"Actions\" -\\> \"Delete security groups\". Confirm.\n",
    "6.  **Empty and Delete S3 Bucket:**\n",
    "      * Go to S3.\n",
    "      * Select `your-aws-account-id-sentiment-model-demo`.\n",
    "      * Click \"Empty\" (follow instructions to confirm).\n",
    "      * Then click \"Delete\" (follow instructions to confirm).\n",
    "7.  **Delete ECR Repositories:**\n",
    "      * Go to ECR.\n",
    "      * Select `sentiment-flask-app` and `sentiment-tfserving-model`.\n",
    "      * Click \"Delete\". Confirm.\n",
    "8.  **Detach IAM Policies (Optional):** If you attached `AmazonS3ReadOnlyAccess` to `ecsTaskExecutionRole`, you might want to detach it if you don't need it for other services.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
